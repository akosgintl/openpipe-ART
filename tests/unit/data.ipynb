{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ff6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f78de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/sky_workdir/tests/unit/wandb/run-20250821_183738-test</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bradhilton/tests/runs/test' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/bradhilton/tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/tests' target=\"_blank\">https://wandb.ai/bradhilton/tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/tests/runs/test' target=\"_blank\">https://wandb.ai/bradhilton/tests/runs/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 18:37:49 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/sky_workdir/src/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore # noqa: F401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 08-21 18:37:59 [__init__.py:235] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.53.2. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.47%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.19 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 368.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.27 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 08-21 18:38:20 [config.py:1604] Using max model len 32768\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 08-21 18:38:21 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":true,\"trace.graph_diagram\":false,\"compile_threads\":26,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":368,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 08-21 18:38:24 [cuda.py:398] Using Flash Attention backend.\n",
      "INFO 08-21 18:38:26 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-21 18:38:26 [model_runner.py:1083] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
      "INFO 08-21 18:38:26 [bitsandbytes_loader.py:733] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 08-21 18:38:27 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 08-21 18:38:42 [weight_utils.py:312] Time spent downloading weights for unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit: 15.237522 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 27.85it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 18:38:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 08-21 18:38:45 [model_runner.py:1115] Model loading took 6.7355 GiB and 17.780424 seconds\n",
      "INFO 08-21 18:38:54 [worker.py:295] Memory profiling takes 8.58 seconds\n",
      "INFO 08-21 18:38:54 [worker.py:295] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.78) = 62.14GiB\n",
      "INFO 08-21 18:38:54 [worker.py:295] model weights take 6.74GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.72GiB; the rest of the memory reserved for KV Cache is 50.54GiB.\n",
      "INFO 08-21 18:38:54 [executor_base.py:113] # cuda blocks: 59142, # CPU blocks: 7021\n",
      "INFO 08-21 18:38:54 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 28.88x\n",
      "INFO 08-21 18:38:58 [vllm_utils.py:671] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 08-21 18:38:58 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:29<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 18:39:27 [model_runner.py:1537] Graph capturing finished in 30 secs, took 1.35 GiB\n",
      "INFO 08-21 18:39:27 [vllm_utils.py:678] Unsloth: Patched vLLM v0 graph capture finished in 30 secs.\n",
      "INFO 08-21 18:39:28 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 43.07 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'k_norm', 'post_feedforward_layernorm', 'q_norm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'k_norm', 'post_feedforward_layernorm', 'q_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from art.local import LocalBackend\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=\"test\", project=\"tests\", base_model=\"Qwen/Qwen2.5-7B-Instruct\"\n",
    ")\n",
    "await model.register(LocalBackend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd4bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = model.openai_client()\n",
    "\n",
    "chat_completion = await client.chat.completions.create(\n",
    "    model=\"test\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hi!\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6803a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-293ce9f37dba40e5be39448acaf6fb49',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': {'content': [{'token': 'token_id:9707',\n",
       "      'bytes': [72, 101, 108, 108, 111],\n",
       "      'logprob': -0.0017243054462596774,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:0',\n",
       "      'bytes': [33],\n",
       "      'logprob': -0.007611795328557491,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:2585',\n",
       "      'bytes': [32, 72, 111, 119],\n",
       "      'logprob': -0.03061593696475029,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:646',\n",
       "      'bytes': [32, 99, 97, 110],\n",
       "      'logprob': -1.1920858014491387e-05,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:358',\n",
       "      'bytes': [32, 73],\n",
       "      'logprob': -2.3841855067985307e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:7789',\n",
       "      'bytes': [32, 97, 115, 115, 105, 115, 116],\n",
       "      'logprob': -0.020548323169350624,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:498',\n",
       "      'bytes': [32, 121, 111, 117],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:3351',\n",
       "      'bytes': [32, 116, 111, 100, 97, 121],\n",
       "      'logprob': -4.410734163684538e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:30',\n",
       "      'bytes': [63],\n",
       "      'logprob': -2.3841855067985307e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:151645',\n",
       "      'bytes': [],\n",
       "      'logprob': -0.0083366259932518,\n",
       "      'top_logprobs': []}],\n",
       "    'refusal': None},\n",
       "   'message': {'content': 'Hello! How can I assist you today?',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'annotations': None,\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': [],\n",
       "    'reasoning_content': None},\n",
       "   'stop_reason': None}],\n",
       " 'created': 1755801745,\n",
       " 'model': 'test',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': None,\n",
       " 'usage': {'completion_tokens': 10,\n",
       "  'prompt_tokens': 31,\n",
       "  'total_tokens': 41,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': None},\n",
       " 'prompt_logprobs': None,\n",
       " 'kv_transfer_params': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.model_dump(mode=\"json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
