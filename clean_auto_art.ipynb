{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OpenPipe/ART/blob/auto-art/clean_auto_art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a model for your custom task, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**Custom Task Training with ART**\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 7B model to perform any single-turn task you describe - no labeled data needed! Simply describe what you want the model to learn, and this notebook will:\n",
        "\n",
        "1. Generate diverse input examples for your task\n",
        "2. Create an appropriate system prompt\n",
        "3. Train the model using RULER's automatic evaluation\n",
        "4. Test the trained model on new inputs\n",
        "\n",
        "RULER learns what makes a good output purely from your task description - no expected outputs required!\n",
        "\n",
        "You will learn how to use RULER for unsupervised learning, define custom [rollouts](https://art.openpipe.ai/resources/glossary#rollout), and run a [training loop](https://art.openpipe.ai/fundamentals/training-loop) that automatically improves your model."
      ],
      "metadata": {
        "id": "caZYLROd8xnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Configuration\"></a>\n",
        "\n",
        "### ðŸŽ¯ Configuration - Edit These Settings\n",
        "\n",
        "Add an OpenRouter key and customize your training by modifying the values below:"
      ],
      "metadata": {
        "id": "D8b8kgQ69ZDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required - Used for generating training inputs and RULER evaluation\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "\n",
        "# Optional - Enables metric logging\n",
        "WANDB_API_KEY = \"\"\n",
        "\n",
        "# Describe your custom task (be specific!)\n",
        "TASK_DESCRIPTION = \"\"\"\n",
        "Convert informal bug reports into structured JIRA-style tickets with these exact sections:\n",
        "- SUMMARY: (one line title)\n",
        "- PRIORITY: (Critical/High/Medium/Low based on impact)\n",
        "- STEPS TO REPRODUCE: (numbered list)\n",
        "- EXPECTED RESULT: (what should happen)\n",
        "- ACTUAL RESULT: (what actually happens)\n",
        "- ENVIRONMENT: (extracted system/version info)\n",
        "\"\"\"\n",
        "\n",
        "# Choose the base model to train\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Options: \"Qwen/Qwen2.5-1.5B-Instruct\", \"Qwen/Qwen2.5-3B-Instruct\", etc."
      ],
      "metadata": {
        "id": "so6r1_OG9en3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Advanced Settings\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"custom-task-model-001\"  # Name for your trained model\n",
        "PROJECT_NAME = \"custom-task-training\"  # Project name for tracking\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_training_inputs\": 25,  # Number of training inputs to generate\n",
        "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
        "    \"num_epochs\": 1,  # Number of times through all data\n",
        "    \"rollouts_per_group\": 4,  # Different responses per input (for RULER comparison)\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"max_training_steps\": None,  # Maximum training steps (set to None for no limit)\n",
        "}\n",
        "\n",
        "# Evaluation configuration\n",
        "RULER_MODEL = \"openrouter/moonshotai/kimi-k2\"  # Model for RULER evaluation\n",
        "INPUT_GENERATION_MODEL=\"openrouter/moonshotai/kimi-k2\"\n",
        "NUM_TEST_INPUTS = 5  # Number of test inputs to generate\n",
        "\n",
        "# GPU configuration (for T4 â€”Â keep these as-is unless you have a reason to change them)\n",
        "MAX_SEQ_LENGTH = 4096  # Maximum sequence length\n",
        "GPU_MEMORY_UTILIZATION = 0.8  # GPU memory usage (0.0-1.0)"
      ],
      "metadata": {
        "id": "I_AFDSOv_LrB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!uv pip install openpipe-art==0.3.11.post2 langchain-core tenacity --prerelease allow --no-cache-dir\n",
        "\n",
        "import os\n",
        "\n",
        "# Required\n",
        "if OPENROUTER_API_KEY:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n",
        "\n",
        "\n",
        "#@title Run this cell to train your model!\n",
        "import json\n",
        "import asyncio\n",
        "from typing import List, Dict, Tuple\n",
        "from pydantic import BaseModel, Field\n",
        "from litellm import acompletion\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TrainingInput(BaseModel):\n",
        "    input: str = Field(description=\"The input text for the task\")\n",
        "\n",
        "class TrainingDataset(BaseModel):\n",
        "    inputs: List[TrainingInput] = Field(description=\"List of training inputs\")\n",
        "\n",
        "async def generate_training_inputs(task_description: str, num_examples: int = 50) -> List[str]:\n",
        "    \"\"\"Generate diverse training inputs for the given task\"\"\"\n",
        "\n",
        "    system_prompt = f\"\"\"You are a helpful assistant that generates diverse, high-quality training inputs.\n",
        "\n",
        "Task: {task_description}\n",
        "\n",
        "Generate {num_examples} diverse INPUT examples that someone might provide for this task.\n",
        "Make sure the inputs:\n",
        "1. Cover a wide range of cases and edge cases\n",
        "2. Are realistic and practical\n",
        "3. Vary in length and complexity\n",
        "4. Represent real-world scenarios\n",
        "\n",
        "Only generate the INPUTS, not the outputs. RULER will evaluate the model's attempts automatically.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Generate {num_examples} input examples for the task described above.\"}\n",
        "    ]\n",
        "\n",
        "    print(f\"Generating {num_examples} training inputs...\")\n",
        "    response = await acompletion(\n",
        "        model=INPUT_GENERATION_MODEL,\n",
        "        messages=messages,\n",
        "        response_format=TrainingDataset,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "    dataset = TrainingDataset.model_validate_json(response.choices[0].message.content)\n",
        "    return [ex.input for ex in dataset.inputs]\n",
        "\n",
        "# Generate training inputs\n",
        "training_inputs = await generate_training_inputs(TASK_DESCRIPTION, num_examples=TRAINING_CONFIG[\"num_training_inputs\"])\n",
        "print(f\"\\nGenerated {len(training_inputs)} training inputs!\")\n",
        "print(\"\\nFirst 5 examples:\")\n",
        "for i, input_text in enumerate(training_inputs[:5]):\n",
        "    print(f\"\\nExample {i+1}: {input_text}\")\n",
        "\n",
        "#@title Model Creation Code\n",
        "import art\n",
        "from art.local import LocalBackend\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_NAME,\n",
        "    project=PROJECT_NAME,\n",
        "    base_model=BASE_MODEL,\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Base model:\", BASE_MODEL)\n",
        "print(\"Model name:\", MODEL_NAME)\n",
        "print(\"Project name:\", PROJECT_NAME)\n",
        "\n",
        "#@title Rollout Function Code\n",
        "\n",
        "import art\n",
        "import weave\n",
        "from litellm import acompletion\n",
        "from art.utils.litellm import convert_litellm_choice_to_openai\n",
        "\n",
        "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
        "    weave.init(PROJECT_NAME, settings={\"print_call_link\": False})\n",
        "\n",
        "# Generate a system prompt for the task\n",
        "async def generate_system_prompt(task_description: str) -> str:\n",
        "    \"\"\"Generate an appropriate system prompt for the task\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Generate a clear, concise system prompt for a model that will perform the following task. The prompt should be direct and instructional.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Task: {task_description}\\n\\nGenerate a system prompt for this task.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=\"openai/gpt-4.1\",\n",
        "        messages=messages,\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "SYSTEM_PROMPT = await generate_system_prompt(TASK_DESCRIPTION)\n",
        "print(f\"Generated system prompt:\\n\\n{SYSTEM_PROMPT}\")\n",
        "\n",
        "class TaskInput(BaseModel):\n",
        "    step: int\n",
        "    input_text: str\n",
        "\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, task_input: TaskInput) -> art.Trajectory:\n",
        "    \"\"\"Execute a single rollout for the custom task\"\"\"\n",
        "\n",
        "    traj = art.Trajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"step\": task_input.step,\n",
        "            \"input\": task_input.input_text,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Build the conversation\n",
        "    traj.messages_and_choices = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": task_input.input_text},\n",
        "    ]\n",
        "\n",
        "    # Get model response\n",
        "    if model.trainable:\n",
        "        litellm_model_name = f\"hosted_vllm/{model.name}\"\n",
        "    else:\n",
        "        litellm_model_name = model.name\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=litellm_model_name,\n",
        "        base_url=model.inference_base_url,\n",
        "        api_key=model.inference_api_key,\n",
        "        temperature=0.7,\n",
        "        messages=traj.messages(),\n",
        "        caching=False,\n",
        "    )\n",
        "\n",
        "    # Add the model's response to the trajectory\n",
        "    traj.messages_and_choices.append(\n",
        "        convert_litellm_choice_to_openai(response.choices[0])\n",
        "    )\n",
        "\n",
        "    return traj\n",
        "\n",
        "print(\"\\nRollout function defined!\")\n",
        "\n",
        "\n",
        "#@title RULER Evaluation Code\n",
        "import art\n",
        "from art.rewards import ruler_score_group\n",
        "\n",
        "# Test RULER with example outputs for a text formalization task\n",
        "test_input = \"hey can u send me the report asap? thx\"\n",
        "\n",
        "base_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Convert informal text to formal business language.\"},\n",
        "    {\"role\": \"user\", \"content\": test_input},\n",
        "]\n",
        "\n",
        "good_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Could you please send me the report at your earliest convenience? Thank you.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "mediocre_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Can you send me the report soon? Thanks.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "bad_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"hey send report quick thx\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "sample_group = art.TrajectoryGroup(\n",
        "    trajectories=[good_trajectory, mediocre_trajectory, bad_trajectory]\n",
        ")\n",
        "\n",
        "# RULER will score these based on how well they accomplish the task\n",
        "judged_group = await ruler_score_group(sample_group, RULER_MODEL, debug=True)\n",
        "assert judged_group is not None\n",
        "\n",
        "# Display rankings\n",
        "sorted_trajectories = sorted(\n",
        "    judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        ")\n",
        "for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "    messages = traj.messages()\n",
        "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "    print(f\"  Response: {messages[-1]['content']}\")\n",
        "\n",
        "\n",
        "#@title Training Loop Code\n",
        "# Training configuration\n",
        "from art.utils import iterate_dataset\n",
        "\n",
        "# Convert training inputs to TaskInput objects\n",
        "training_task_inputs = [\n",
        "    TaskInput(step=0, input_text=inp)\n",
        "    for inp in training_inputs\n",
        "]\n",
        "\n",
        "# Create training iterator\n",
        "training_iterator = iterate_dataset(\n",
        "    training_task_inputs,\n",
        "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
        "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "\n",
        "print(f\"Starting training with {len(training_task_inputs)} inputs...\")\n",
        "print(f\"Training for {TRAINING_CONFIG['num_epochs']} epoch(s)\")\n",
        "print(f\"Generating {TRAINING_CONFIG['rollouts_per_group']} responses per input for RULER to compare\")\n",
        "print(f\"\\nWhy multiple responses? RULER needs to compare different attempts to learn what's good!\")\n",
        "\n",
        "for batch, epoch, global_step, epoch_step in training_iterator:\n",
        "    print(f\"\\nTraining step {global_step}, epoch {epoch}, epoch step {epoch_step}\")\n",
        "    print(f\"Batch contains {len(batch)} inputs\")\n",
        "\n",
        "    # Create trajectory groups for this batch\n",
        "    groups = []\n",
        "    for task_input in batch:\n",
        "        # Update step number\n",
        "        task_input.step = global_step\n",
        "\n",
        "        # Generate multiple responses for each input (RULER will compare these)\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    rollout(model, task_input)\n",
        "                    for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"Generating responses\",\n",
        "        max_exceptions=TRAINING_CONFIG[\"rollouts_per_group\"] * len(batch),\n",
        "    )\n",
        "\n",
        "    # Use RULER to score each group\n",
        "    judged_groups = []\n",
        "    for group in finished_groups:\n",
        "        judged_group = await ruler_score_group(\n",
        "            group,\n",
        "            RULER_MODEL,\n",
        "            debug=False\n",
        "        )\n",
        "        judged_groups.append(judged_group)\n",
        "\n",
        "    # Train on the scored trajectories\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        judged_groups,\n",
        "        config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]),\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )\n",
        "\n",
        "    print(f\"Completed training step {global_step}\")\n",
        "\n",
        "    # Stop after configured steps (if limit is set)\n",
        "    if TRAINING_CONFIG[\"max_training_steps\"] and global_step >= TRAINING_CONFIG[\"max_training_steps\"]:\n",
        "        print(f\"Reached maximum training steps ({TRAINING_CONFIG['max_training_steps']})\")\n",
        "        break\n",
        "\n",
        "print(\"\\nâœ… Training completed!\")"
      ],
      "metadata": {
        "id": "FQ2BfxTUKvgw",
        "outputId": "f1aa86a4-89b7-4834-9528-14a4825c45e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot run the event loop while another loop is running",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mHandler\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masyncio\u001b[0m \u001b[0mautoawait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \"\"\"\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    592\u001b[0m                 'Cannot run the event loop while another loop is running')\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot run the event loop while another loop is running"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YRO9ndqo5ky4",
        "cellView": "form",
        "outputId": "92f7372d-e5cf-4594-c5af-24d59db101d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating test inputs...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate_training_inputs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-747283100.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Generate test inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating test inputs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mgenerate_training_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASK_DESCRIPTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TEST_INPUTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nðŸ§ª Testing the trained model on {len(test_inputs)} new inputs:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_training_inputs' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Test Your Model!\n",
        "\n",
        "# Generate test inputs\n",
        "print(\"Generating test inputs...\")\n",
        "test_inputs = await generate_training_inputs(TASK_DESCRIPTION, num_examples=NUM_TEST_INPUTS)\n",
        "\n",
        "print(f\"\\nðŸ§ª Testing the trained model on {len(test_inputs)} new inputs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, test_input in enumerate(test_inputs):\n",
        "    print(f\"\\nTest {i+1}:\")\n",
        "    print(f\"Input: {test_input}\")\n",
        "\n",
        "    # Run the model\n",
        "    test_task_input = TaskInput(\n",
        "        step=999,\n",
        "        input_text=test_input\n",
        "    )\n",
        "    result_trajectory = await rollout(model, test_task_input)\n",
        "\n",
        "    # Extract the model's response\n",
        "    messages = result_trajectory.messages()\n",
        "    model_response = messages[-1]['content'] if messages else \"No response\"\n",
        "\n",
        "    print(f\"Model output: {model_response}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Testing completed!\")\n",
        "print(f\"\\nYour model '{MODEL_NAME}' has been trained to: {TASK_DESCRIPTION}\")\n",
        "print(\"\\nTo use this model in production:\")\n",
        "print(\"1. The model checkpoint is saved in ./.art/\")\n",
        "print(\"2. You can load it using the vLLM library\")\n",
        "print(\"3. Or continue training with more examples by adjusting the configuration at the top\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a custom model for your task using only:\n",
        "- A task description\n",
        "- Example inputs (no outputs needed!)\n",
        "- RULER's automatic evaluation\n",
        "\n",
        "Here are some ways to improve results:\n",
        "\n",
        "1. **More diverse inputs**: Generate more varied input examples\n",
        "2. **Longer training**: Increase the number of training steps\n",
        "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
        "4. **Task refinement**: Make your task description more specific and detailed\n",
        "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
        "\n",
        "Remember: RULER learns what \"good\" means from your task description alone - no labeled data required!\n",
        "\n",
        "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai)."
      ],
      "metadata": {
        "id": "FuevYgXT-I1h"
      }
    }
  ]
}