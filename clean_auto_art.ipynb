{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OpenPipe/ART/blob/auto-art/clean_auto_art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a model for your custom task, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**Custom Task Training with ART**\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 7B model to perform any single-turn task you describe - no labeled data needed! Simply describe what you want the model to learn, and this notebook will:\n",
        "\n",
        "1. Generate diverse input examples for your task\n",
        "2. Create an appropriate system prompt\n",
        "3. Train the model using RULER's automatic evaluation\n",
        "4. Test the trained model on new inputs\n",
        "\n",
        "RULER learns what makes a good output purely from your task description - no expected outputs required!\n",
        "\n",
        "You will learn how to use RULER for unsupervised learning, define custom [rollouts](https://art.openpipe.ai/resources/glossary#rollout), and run a [training loop](https://art.openpipe.ai/fundamentals/training-loop) that automatically improves your model."
      ],
      "metadata": {
        "id": "caZYLROd8xnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installation\n",
        "\n",
        "%%capture\n",
        "!uv pip install openpipe-art==0.3.11.post2 langchain-core tenacity --prerelease allow --no-cache-dir"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OsrwCDQ5cviC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Configuration\"></a>\n",
        "\n",
        "### ðŸŽ¯ Configuration - Edit These Settings\n",
        "\n",
        "Add an OpenRouter key and customize your training by modifying the values below:"
      ],
      "metadata": {
        "id": "D8b8kgQ69ZDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required - Used for generating training inputs and RULER evaluation\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "\n",
        "# Optional - Enables metric logging\n",
        "WANDB_API_KEY = \"\"\n",
        "\n",
        "# Describe your custom task (be specific!)\n",
        "TASK_DESCRIPTION = \"\"\"\n",
        "Convert informal bug reports into structured JIRA-style tickets with these exact sections:\n",
        "- SUMMARY: (one line title)\n",
        "- PRIORITY: (Critical/High/Medium/Low based on impact)\n",
        "- STEPS TO REPRODUCE: (numbered list)\n",
        "- EXPECTED RESULT: (what should happen)\n",
        "- ACTUAL RESULT: (what actually happens)\n",
        "- ENVIRONMENT: (extracted system/version info)\n",
        "\"\"\"\n",
        "\n",
        "# Choose the base model to train\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Options: \"Qwen/Qwen2.5-1.5B-Instruct\", \"Qwen/Qwen2.5-3B-Instruct\", etc."
      ],
      "metadata": {
        "id": "so6r1_OG9en3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Advanced Settings\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"custom-task-model-001\"  # Name for your trained model\n",
        "PROJECT_NAME = \"custom-task-training\"  # Project name for tracking\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_training_inputs\": 25,  # Number of training inputs to generate\n",
        "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
        "    \"num_epochs\": 1,  # Number of times through all data\n",
        "    \"rollouts_per_group\": 4,  # Different responses per input (for RULER comparison)\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"max_training_steps\": None,  # Maximum training steps (set to None for no limit)\n",
        "}\n",
        "\n",
        "# Evaluation configuration\n",
        "RULER_MODEL = \"openrouter/moonshotai/kimi-k2\"  # Model for RULER evaluation\n",
        "SYSTEM_PROMPT_GENERATION_MODEL=\"openrouter/moonshotai/kimi-k2\"\n",
        "INPUT_GENERATION_MODEL=\"openrouter/moonshotai/kimi-k2\"\n",
        "NUM_TEST_INPUTS = 5  # Number of test inputs to generate\n",
        "\n",
        "# GPU configuration (for T4 â€”Â keep these as-is unless you have a reason to change them)\n",
        "MAX_SEQ_LENGTH = 4096  # Maximum sequence length\n",
        "GPU_MEMORY_UTILIZATION = 0.8  # GPU memory usage (0.0-1.0)"
      ],
      "metadata": {
        "id": "I_AFDSOv_LrB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Required\n",
        "if OPENROUTER_API_KEY:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n",
        "\n",
        "\n",
        "#@title Run this cell to train your model!\n",
        "import json\n",
        "import asyncio\n",
        "from typing import List, Dict, Tuple\n",
        "from pydantic import BaseModel, Field\n",
        "from litellm import acompletion\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TrainingInput(BaseModel):\n",
        "    input: str = Field(description=\"The input text for the task\")\n",
        "\n",
        "class TrainingDataset(BaseModel):\n",
        "    inputs: List[TrainingInput] = Field(description=\"List of training inputs\")\n",
        "\n",
        "async def generate_training_inputs(task_description: str, num_examples: int = 50) -> List[str]:\n",
        "    \"\"\"Generate diverse training inputs for the given task\"\"\"\n",
        "\n",
        "    system_prompt = f\"\"\"You are a helpful assistant that generates diverse, high-quality training inputs.\n",
        "\n",
        "Task: {task_description}\n",
        "\n",
        "Generate {num_examples} diverse INPUT examples that someone might provide for this task.\n",
        "Make sure the inputs:\n",
        "1. Cover a wide range of cases and edge cases\n",
        "2. Are realistic and practical\n",
        "3. Vary in length and complexity\n",
        "4. Represent real-world scenarios\n",
        "\n",
        "Only generate the INPUTS, not the outputs. RULER will evaluate the model's attempts automatically.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Generate {num_examples} input examples for the task described above.\"}\n",
        "    ]\n",
        "\n",
        "    print(f\"Generating {num_examples} training inputs...\")\n",
        "    response = await acompletion(\n",
        "        model=INPUT_GENERATION_MODEL,\n",
        "        messages=messages,\n",
        "        response_format=TrainingDataset,\n",
        "        temperature=0.4,\n",
        "    )\n",
        "\n",
        "    print(\"messages[0]\", messages[0])\n",
        "\n",
        "    print(\"response\", response)\n",
        "\n",
        "    dataset = TrainingDataset.model_validate_json(response.choices[0].message.content)\n",
        "\n",
        "    print(\"dataset\", dataset)\n",
        "    return [ex.input for ex in dataset.inputs]\n",
        "\n",
        "# Generate training inputs\n",
        "\n",
        "print('TRAINING_CONFIG[\"num_training_inputs\"]', TRAINING_CONFIG[\"num_training_inputs\"])\n",
        "\n",
        "training_inputs = await generate_training_inputs(TASK_DESCRIPTION, num_examples=TRAINING_CONFIG[\"num_training_inputs\"])\n",
        "print(f\"\\nGenerated {len(training_inputs)} training inputs!\")\n",
        "print(\"\\nFirst 5 examples:\")\n",
        "for i, input_text in enumerate(training_inputs[:5]):\n",
        "    print(f\"\\nExample {i+1}: {input_text}\")\n",
        "\n",
        "#@title Model Creation Code\n",
        "import art\n",
        "from art.local import LocalBackend\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_NAME,\n",
        "    project=PROJECT_NAME,\n",
        "    base_model=BASE_MODEL,\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Base model:\", BASE_MODEL)\n",
        "print(\"Model name:\", MODEL_NAME)\n",
        "print(\"Project name:\", PROJECT_NAME)\n",
        "\n",
        "#@title Rollout Function Code\n",
        "\n",
        "import art\n",
        "import weave\n",
        "from litellm import acompletion\n",
        "from art.utils.litellm import convert_litellm_choice_to_openai\n",
        "\n",
        "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
        "    weave.init(PROJECT_NAME, settings={\"print_call_link\": False})\n",
        "\n",
        "# Generate a system prompt for the task\n",
        "async def generate_system_prompt(task_description: str) -> str:\n",
        "    \"\"\"Generate an appropriate system prompt for the task\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Generate a clear, concise system prompt for a model that will perform the following task. The prompt should be direct and instructional.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Task: {task_description}\\n\\nGenerate a system prompt for this task.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=SYSTEM_PROMPT_GENERATION_MODEL,\n",
        "        messages=messages,\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "SYSTEM_PROMPT = await generate_system_prompt(TASK_DESCRIPTION)\n",
        "print(f\"Generated system prompt:\\n\\n{SYSTEM_PROMPT}\")\n",
        "\n",
        "class TaskInput(BaseModel):\n",
        "    step: int\n",
        "    input_text: str\n",
        "\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, task_input: TaskInput) -> art.Trajectory:\n",
        "    \"\"\"Execute a single rollout for the custom task\"\"\"\n",
        "\n",
        "    traj = art.Trajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"step\": task_input.step,\n",
        "            \"input\": task_input.input_text,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Build the conversation\n",
        "    traj.messages_and_choices = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": task_input.input_text},\n",
        "    ]\n",
        "\n",
        "    # Get model response\n",
        "    if model.trainable:\n",
        "        litellm_model_name = f\"hosted_vllm/{model.name}\"\n",
        "    else:\n",
        "        litellm_model_name = model.name\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=litellm_model_name,\n",
        "        base_url=model.inference_base_url,\n",
        "        api_key=model.inference_api_key,\n",
        "        temperature=0.7,\n",
        "        messages=traj.messages(),\n",
        "        caching=False,\n",
        "    )\n",
        "\n",
        "    # Add the model's response to the trajectory\n",
        "    traj.messages_and_choices.append(\n",
        "        convert_litellm_choice_to_openai(response.choices[0])\n",
        "    )\n",
        "\n",
        "    return traj\n",
        "\n",
        "print(\"\\nRollout function defined!\")\n",
        "\n",
        "\n",
        "import art\n",
        "from art.rewards import ruler_score_group\n",
        "\n",
        "# Test RULER with example outputs for a text formalization task\n",
        "test_input = \"hey can u send me the report asap? thx\"\n",
        "\n",
        "base_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Convert informal text to formal business language.\"},\n",
        "    {\"role\": \"user\", \"content\": test_input},\n",
        "]\n",
        "\n",
        "good_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Could you please send me the report at your earliest convenience? Thank you.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "mediocre_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Can you send me the report soon? Thanks.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "bad_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"hey send report quick thx\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "sample_group = art.TrajectoryGroup(\n",
        "    trajectories=[good_trajectory, mediocre_trajectory, bad_trajectory]\n",
        ")\n",
        "\n",
        "# RULER will score these based on how well they accomplish the task\n",
        "judged_group = await ruler_score_group(sample_group, RULER_MODEL, debug=True)\n",
        "assert judged_group is not None\n",
        "\n",
        "# Display rankings\n",
        "sorted_trajectories = sorted(\n",
        "    judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        ")\n",
        "for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "    messages = traj.messages()\n",
        "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "    print(f\"  Response: {messages[-1]['content']}\")\n",
        "\n",
        "\n",
        "#@title Training Loop Code\n",
        "# Training configuration\n",
        "from art.utils import iterate_dataset\n",
        "\n",
        "# Convert training inputs to TaskInput objects\n",
        "training_task_inputs = [\n",
        "    TaskInput(step=0, input_text=inp)\n",
        "    for inp in training_inputs\n",
        "]\n",
        "\n",
        "# Create training iterator\n",
        "training_iterator = iterate_dataset(\n",
        "    training_task_inputs,\n",
        "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
        "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "\n",
        "print(f\"Starting training with {len(training_task_inputs)} inputs...\")\n",
        "print(f\"Training for {TRAINING_CONFIG['num_epochs']} epoch(s)\")\n",
        "print(f\"Generating {TRAINING_CONFIG['rollouts_per_group']} responses per input for RULER to compare\")\n",
        "print(f\"\\nWhy multiple responses? RULER needs to compare different attempts to learn what's good!\")\n",
        "\n",
        "for batch, epoch, global_step, epoch_step in training_iterator:\n",
        "    print(f\"\\nTraining step {global_step}, epoch {epoch}, epoch step {epoch_step}\")\n",
        "    print(f\"Batch contains {len(batch)} inputs\")\n",
        "\n",
        "    # Create trajectory groups for this batch\n",
        "    groups = []\n",
        "    for task_input in batch:\n",
        "        # Update step number\n",
        "        task_input.step = global_step\n",
        "\n",
        "        # Generate multiple responses for each input (RULER will compare these)\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    rollout(model, task_input)\n",
        "                    for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"Generating responses\",\n",
        "        max_exceptions=TRAINING_CONFIG[\"rollouts_per_group\"] * len(batch),\n",
        "    )\n",
        "\n",
        "    # Use RULER to score each group\n",
        "    judged_groups = []\n",
        "    for group in finished_groups:\n",
        "        judged_group = await ruler_score_group(\n",
        "            group,\n",
        "            RULER_MODEL,\n",
        "            debug=False\n",
        "        )\n",
        "        judged_groups.append(judged_group)\n",
        "\n",
        "    # Train on the scored trajectories\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        judged_groups,\n",
        "        config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]),\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )\n",
        "\n",
        "    print(f\"Completed training step {global_step}\")\n",
        "\n",
        "    # Stop after configured steps (if limit is set)\n",
        "    if TRAINING_CONFIG[\"max_training_steps\"] and global_step >= TRAINING_CONFIG[\"max_training_steps\"]:\n",
        "        print(f\"Reached maximum training steps ({TRAINING_CONFIG['max_training_steps']})\")\n",
        "        break\n",
        "\n",
        "print(\"\\nâœ… Training completed!\")"
      ],
      "metadata": {
        "id": "FQ2BfxTUKvgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce6dcc14-856f-48c8-a48a-d0a79f66c9c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\n",
            "TRAINING_CONFIG[\"num_training_inputs\"] 25\n",
            "Generating 25 training inputs...\n",
            "messages[0] {'role': 'system', 'content': \"You are a helpful assistant that generates diverse, high-quality training inputs.\\n\\nTask: \\nConvert informal bug reports into structured JIRA-style tickets with these exact sections:\\n- SUMMARY: (one line title)\\n- PRIORITY: (Critical/High/Medium/Low based on impact)\\n- STEPS TO REPRODUCE: (numbered list)\\n- EXPECTED RESULT: (what should happen)\\n- ACTUAL RESULT: (what actually happens)\\n- ENVIRONMENT: (extracted system/version info)\\n\\n\\nGenerate 25 diverse INPUT examples that someone might provide for this task.\\nMake sure the inputs:\\n1. Cover a wide range of cases and edge cases\\n2. Are realistic and practical\\n3. Vary in length and complexity\\n4. Represent real-world scenarios\\n\\nOnly generate the INPUTS, not the outputs. RULER will evaluate the model's attempts automatically.\\n\"}\n",
            "response ModelResponse(id='gen-1753747332-QW9wpvru0ul0Bpqg7l89', created=1753747333, model='moonshotai/kimi-k2', object='chat.completion', system_fingerprint='fpv0_170758dd', choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"inputs\":[\\n  {\"input\":\"the stupid search bar keeps giving me zero results even when i type exact product names. i tried it on my iphone 12 running ios 16.3 and also on my macbook air m1 with safari 15.6. this is really frustrating because customers cant find anything\"},\\n  \\n  {\"input\":\"URGENT: Payment gateway throwing 500 error after latest deploy. Stripe webhook failing with timeout. Happening on prod since 2:30pm EST. Using Rails 7.0.4, Ruby 3.1.2, deployed on Heroku-22 stack. Customers can\\'t complete purchases\"},\\n  \\n  {\"input\":\"android app crashes when uploading profile pic from gallery on pixel 6 android 13. works fine with camera. just updated to app version 3.2.1 yesterday. get the error \\\\\"java.lang.OutOfMemoryError\\\\\" in logcat\"},\\n  \\n  {\"input\":\"login with google oauth broken on staging. after clicking \\'sign in with google\\' it redirects to localhost:3000 instead of staging url. using nextjs 13.4.1 with next-auth 4.22.1. started after we added the new environment variables\"},\\n  \\n  {\"input\":\"database connection pool exhaustion happening every morning at 9am sharp. postgres 14.5 on aws rds db.t3.medium. seeing \\'FATAL: remaining connection slots are reserved\\' errors. using sequelize 6.32.1 with default pool settings\"},\\n  \\n  {\"input\":\"email notifications not sending to users with + in their email address like john+test@gmail.com. using sendgrid integration on node.js 18.17.0. regular emails work fine. started last week after we updated the email validation regex\"},\\n  \\n  {\"input\":\"the dark mode toggle doesn\\'t save preference. every time i refresh the page it goes back to light mode. using chrome 115.0.5790.114 on windows 11. local storage shows the setting but it\\'s not being read on page load\"},\\n  \\n  {\"input\":\"csv export is generating corrupted files for datasets over 10k rows. excel says \\'file format or extension is not valid\\'. using pandas 2.0.3 with python 3.11.4. works fine for smaller datasets. encoding seems to be utf-8\"},\\n  \\n  {\"input\":\"websocket connection drops exactly every 5 minutes in our real-time chat app. using socket.io 4.7.2 with nginx reverse proxy on ubuntu 22.04. client shows \\'transport close\\' error. happens on both firefox and chrome\"},\\n  \\n  {\"input\":\"the date picker calendar shows wrong month names in french locale. instead of \\'janvier\\' it shows \\'january\\'. using react-datepicker 4.16.0 with moment 2.29.4. other translations work fine, just month names are broken\"},\\n  \\n  {\"input\":\"memory leak in the dashboard when leaving it open overnight. chrome tab goes from 200mb to 2gb in 8 hours. using vue 3.3.4 with chart.js 4.3.0. noticed setinterval keeps running even after component unmount\"},\\n  \\n  {\"input\":\"file upload fails for .heic images from iphone. server responds with \\'unsupported file type\\' but works for jpg/png. using multer 1.4.5 with express 4.18.2. need to add heic support or better error message\"},\\n  \\n  {\"input\":\"search autocomplete is super slow, takes 5+ seconds to show suggestions. using elasticsearch 8.8.0 with 100k documents. query is doing wildcard search on multiple fields. server cpu spikes to 100% during searches\"},\\n  \\n  {\"input\":\"the \\'remember me\\' checkbox does nothing. still get logged out after 2 hours. using laravel 10.10 with sanctum for api tokens. session lifetime is set to 43200 minutes but it\\'s not working\"},\\n  \\n  {\"input\":\"api rate limiting is blocking legitimate users. getting 429 errors after just 10 requests. using nginx limit_req with 1r/s burst=5. mobile app makes parallel requests on startup causing immediate blocks\"},\\n  \\n  {\"input\":\"the pdf export cuts off content at page breaks. tables are getting split mid-row. using puppeteer 20.7.2 with chrome headless. css print styles are set but @page margins seem ignored\"},\\n  \\n  {\"input\":\"user avatars showing as broken images for users who signed up with facebook. urls are https but getting \\'net::ERR_CERT_AUTHORITY_INVALID\\'. using facebook graph api v18.0. started after ssl cert renewal\"},\\n  \\n  {\"input\":\"the bulk delete feature is only deleting first 100 items even when selecting \\'all\\'. using datatables 1.13.5 with server-side processing. the ajax request only sends visible page ids instead of all selected\"},\\n  \\n  {\"input\":\"timezone conversion is wrong for users in arizona. showing times 1 hour ahead. using moment-timezone 0.5.43. arizona doesn\\'t do daylight saving but system thinks it does. affects all scheduling features\"},\\n  \\n  {\"input\":\"the loading spinner spins forever on the analytics page. network tab shows api call returning 200 with data. using swr 2.2.0 with nextjs. console shows \\'cannot read property of undefined\\' in the data transformation\"},\\n  \\n  {\"input\":\"push notifications not working on ios 16.5.1. permission dialog shows but no notifications received. using firebase cloud messaging with react-native 0.72.3. android works fine. apns certificate expires in 2025\"},\\n  \\n  {\"input\":\"the markdown editor preview doesn\\'t render tables correctly. pipes are showing as plain text. using marked 5.1.1 with custom renderer. github flavored markdown is enabled in settings\"},\\n  \\n  {\"input\":\"user registration fails with \\'email already exists\\' even for new emails. checked database - no duplicates. using mongodb 6.0.8 with case-insensitive unique index. happens with emails containing dots like john.doe@gmail.com\"},\\n  \\n  {\"input\":\"the infinite scroll keeps loading same posts repeatedly. scroll position jumps back to top. using intersection observer api with react 18.2.0. state management with redux toolkit. pagination cursor not updating\"},\\n  \\n  {\"input\":\"docker container exits with code 137 every night at 3am. memory limit is 512mb but usage only shows 300mb. using node:18-alpine image with pm2. logs show \\'killed\\' message. health check endpoint returns 200\"}\\n]}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None, 'reasoning': None}), provider_specific_fields={'native_finish_reason': 'stop'})], usage=Usage(completion_tokens=1366, prompt_tokens=197, total_tokens=1563, completion_tokens_details=None, prompt_tokens_details=None), provider='Moonshot AI')\n",
            "dataset inputs=[TrainingInput(input='the stupid search bar keeps giving me zero results even when i type exact product names. i tried it on my iphone 12 running ios 16.3 and also on my macbook air m1 with safari 15.6. this is really frustrating because customers cant find anything'), TrainingInput(input=\"URGENT: Payment gateway throwing 500 error after latest deploy. Stripe webhook failing with timeout. Happening on prod since 2:30pm EST. Using Rails 7.0.4, Ruby 3.1.2, deployed on Heroku-22 stack. Customers can't complete purchases\"), TrainingInput(input='android app crashes when uploading profile pic from gallery on pixel 6 android 13. works fine with camera. just updated to app version 3.2.1 yesterday. get the error \"java.lang.OutOfMemoryError\" in logcat'), TrainingInput(input=\"login with google oauth broken on staging. after clicking 'sign in with google' it redirects to localhost:3000 instead of staging url. using nextjs 13.4.1 with next-auth 4.22.1. started after we added the new environment variables\"), TrainingInput(input=\"database connection pool exhaustion happening every morning at 9am sharp. postgres 14.5 on aws rds db.t3.medium. seeing 'FATAL: remaining connection slots are reserved' errors. using sequelize 6.32.1 with default pool settings\"), TrainingInput(input='email notifications not sending to users with + in their email address like john+test@gmail.com. using sendgrid integration on node.js 18.17.0. regular emails work fine. started last week after we updated the email validation regex'), TrainingInput(input=\"the dark mode toggle doesn't save preference. every time i refresh the page it goes back to light mode. using chrome 115.0.5790.114 on windows 11. local storage shows the setting but it's not being read on page load\"), TrainingInput(input=\"csv export is generating corrupted files for datasets over 10k rows. excel says 'file format or extension is not valid'. using pandas 2.0.3 with python 3.11.4. works fine for smaller datasets. encoding seems to be utf-8\"), TrainingInput(input=\"websocket connection drops exactly every 5 minutes in our real-time chat app. using socket.io 4.7.2 with nginx reverse proxy on ubuntu 22.04. client shows 'transport close' error. happens on both firefox and chrome\"), TrainingInput(input=\"the date picker calendar shows wrong month names in french locale. instead of 'janvier' it shows 'january'. using react-datepicker 4.16.0 with moment 2.29.4. other translations work fine, just month names are broken\"), TrainingInput(input='memory leak in the dashboard when leaving it open overnight. chrome tab goes from 200mb to 2gb in 8 hours. using vue 3.3.4 with chart.js 4.3.0. noticed setinterval keeps running even after component unmount'), TrainingInput(input=\"file upload fails for .heic images from iphone. server responds with 'unsupported file type' but works for jpg/png. using multer 1.4.5 with express 4.18.2. need to add heic support or better error message\"), TrainingInput(input='search autocomplete is super slow, takes 5+ seconds to show suggestions. using elasticsearch 8.8.0 with 100k documents. query is doing wildcard search on multiple fields. server cpu spikes to 100% during searches'), TrainingInput(input=\"the 'remember me' checkbox does nothing. still get logged out after 2 hours. using laravel 10.10 with sanctum for api tokens. session lifetime is set to 43200 minutes but it's not working\"), TrainingInput(input='api rate limiting is blocking legitimate users. getting 429 errors after just 10 requests. using nginx limit_req with 1r/s burst=5. mobile app makes parallel requests on startup causing immediate blocks'), TrainingInput(input='the pdf export cuts off content at page breaks. tables are getting split mid-row. using puppeteer 20.7.2 with chrome headless. css print styles are set but @page margins seem ignored'), TrainingInput(input=\"user avatars showing as broken images for users who signed up with facebook. urls are https but getting 'net::ERR_CERT_AUTHORITY_INVALID'. using facebook graph api v18.0. started after ssl cert renewal\"), TrainingInput(input=\"the bulk delete feature is only deleting first 100 items even when selecting 'all'. using datatables 1.13.5 with server-side processing. the ajax request only sends visible page ids instead of all selected\"), TrainingInput(input=\"timezone conversion is wrong for users in arizona. showing times 1 hour ahead. using moment-timezone 0.5.43. arizona doesn't do daylight saving but system thinks it does. affects all scheduling features\"), TrainingInput(input=\"the loading spinner spins forever on the analytics page. network tab shows api call returning 200 with data. using swr 2.2.0 with nextjs. console shows 'cannot read property of undefined' in the data transformation\"), TrainingInput(input='push notifications not working on ios 16.5.1. permission dialog shows but no notifications received. using firebase cloud messaging with react-native 0.72.3. android works fine. apns certificate expires in 2025'), TrainingInput(input=\"the markdown editor preview doesn't render tables correctly. pipes are showing as plain text. using marked 5.1.1 with custom renderer. github flavored markdown is enabled in settings\"), TrainingInput(input=\"user registration fails with 'email already exists' even for new emails. checked database - no duplicates. using mongodb 6.0.8 with case-insensitive unique index. happens with emails containing dots like john.doe@gmail.com\"), TrainingInput(input='the infinite scroll keeps loading same posts repeatedly. scroll position jumps back to top. using intersection observer api with react 18.2.0. state management with redux toolkit. pagination cursor not updating'), TrainingInput(input=\"docker container exits with code 137 every night at 3am. memory limit is 512mb but usage only shows 300mb. using node:18-alpine image with pm2. logs show 'killed' message. health check endpoint returns 200\")]\n",
            "\n",
            "Generated 25 training inputs!\n",
            "\n",
            "First 5 examples:\n",
            "\n",
            "Example 1: the stupid search bar keeps giving me zero results even when i type exact product names. i tried it on my iphone 12 running ios 16.3 and also on my macbook air m1 with safari 15.6. this is really frustrating because customers cant find anything\n",
            "\n",
            "Example 2: URGENT: Payment gateway throwing 500 error after latest deploy. Stripe webhook failing with timeout. Happening on prod since 2:30pm EST. Using Rails 7.0.4, Ruby 3.1.2, deployed on Heroku-22 stack. Customers can't complete purchases\n",
            "\n",
            "Example 3: android app crashes when uploading profile pic from gallery on pixel 6 android 13. works fine with camera. just updated to app version 3.2.1 yesterday. get the error \"java.lang.OutOfMemoryError\" in logcat\n",
            "\n",
            "Example 4: login with google oauth broken on staging. after clicking 'sign in with google' it redirects to localhost:3000 instead of staging url. using nextjs 13.4.1 with next-auth 4.22.1. started after we added the new environment variables\n",
            "\n",
            "Example 5: database connection pool exhaustion happening every morning at 9am sharp. postgres 14.5 on aws rds db.t3.medium. seeing 'FATAL: remaining connection slots are reserved' errors. using sequelize 6.32.1 with default pool settings\n",
            "==((====))==  Unsloth 2025.5.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.25%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 192.\n",
            "Unsloth: vLLM's KV Cache can use up to 5.67 GB. Also swap space = 2 GB.\n",
            "WARNING 07-29 00:03:56 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Model architectures ['Qwen2ForCausalLM'] failed to be inspected. Please check the logs for more details.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/vllm_utils.py\u001b[0m in \u001b[0;36mload_vllm\u001b[0;34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, return_args)\u001b[0m\n\u001b[1;32m   1136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_async\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m                 \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAsyncLLMEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_engine_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAsyncEngineArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0muse_engine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/local/state.py\u001b[0m in \u001b[0;36m_from_engine_args\u001b[0;34m(engine_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mengine_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAsyncEngineArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mengine_args_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_engine_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/local/state.py\u001b[0m in \u001b[0;36m_from_engine_args\u001b[0;34m(engine_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mengine_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAsyncEngineArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mengine_args_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_engine_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/local/state.py\u001b[0m in \u001b[0;36m_from_engine_args\u001b[0;34m(engine_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mengine_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAsyncEngineArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mengine_args_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_engine_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/local/state.py\u001b[0m in \u001b[0;36m_from_engine_args\u001b[0;34m(engine_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mengine_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAsyncEngineArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mengine_args_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_engine_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/async_llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, start_engine_loop, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mvllm_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_engine_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\u001b[0m in \u001b[0;36mcreate_engine_config\u001b[0;34m(self, usage_context)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mdevice_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeviceConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\u001b[0m in \u001b[0;36mcreate_model_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         return ModelConfig(\n\u001b[0m\u001b[1;32m    988\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/config.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, tokenizer, tokenizer_mode, trust_remote_code, dtype, seed, hf_config_path, allowed_local_media_path, revision, code_revision, rope_scaling, rope_theta, tokenizer_revision, max_model_len, spec_target_max_model_len, quantization, enforce_eager, max_seq_len_to_capture, max_logprobs, disable_sliding_window, disable_cascade_attn, skip_tokenizer_init, served_model_name, limit_mm_per_prompt, use_async_output_proc, config_format, hf_token, hf_overrides, mm_processor_kwargs, disable_mm_preprocessor_cache, override_neuron_config, override_pooler_config, logits_processor_pattern, generation_config, enable_sleep_mode, override_generation_config, model_impl)\u001b[0m\n\u001b[1;32m    516\u001b[0m                                                        served_model_name)\n\u001b[0;32m--> 517\u001b[0;31m         self.multimodal_config = self._init_multimodal_config(\n\u001b[0m\u001b[1;32m    518\u001b[0m             limit_mm_per_prompt)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/config.py\u001b[0m in \u001b[0;36m_init_multimodal_config\u001b[0;34m(self, limit_mm_per_prompt)\u001b[0m\n\u001b[1;32m    585\u001b[0m     ) -> Optional[\"MultiModalConfig\"]:\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_multimodal_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitectures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMultiModalConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit_per_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_mm_per_prompt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/registry.py\u001b[0m in \u001b[0;36mis_multimodal_model\u001b[0;34m(self, architectures)\u001b[0m\n\u001b[1;32m    504\u001b[0m     ) -> bool:\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mmodel_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspect_model_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitectures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_multimodal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/registry.py\u001b[0m in \u001b[0;36minspect_model_cls\u001b[0;34m(self, architectures)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_for_unsupported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitectures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/registry.py\u001b[0m in \u001b[0;36m_raise_for_unsupported\u001b[0;34m(self, architectures)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_supported_archs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0march\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    416\u001b[0m                 \u001b[0;34mf\"Model architectures {architectures} failed \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model architectures ['Qwen2ForCausalLM'] failed to be inspected. Please check the logs for more details.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-3152106333.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Register the model with the local Backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model created!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/model.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, backend, _openai_client_config)\u001b[0m\n\u001b[1;32m    188\u001b[0m     ) -> None:\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         base_url, api_key = await backend._prepare_backend_for_training(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_openai_client_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/local/backend.py\u001b[0m in \u001b[0;36m_prepare_backend_for_training\u001b[0;34m(self, model, config)\u001b[0m\n\u001b[1;32m    218\u001b[0m     ) -> tuple[str, str]:\n\u001b[1;32m    219\u001b[0m         \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_service\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_openai_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mserver_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"server_args\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/local/service.py\u001b[0m in \u001b[0;36mstart_openai_server\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlora_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mlora_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self.output_dir}/0000\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_openai_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         self._openai_server_task = await openai_server_task(\n",
            "\u001b[0;32m/usr/lib/python3.11/functools.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/local/service.py\u001b[0m in \u001b[0;36mstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mModelState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/local/state.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m         self.model, self.tokenizer = cast(\n\u001b[1;32m     79\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCausallLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedTokenizerBase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0munsloth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_args\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         )\n\u001b[1;32m     82\u001b[0m         \u001b[0mAsyncLLMEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_engine_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_engine_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/qwen2.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     ):\n\u001b[0;32m---> 87\u001b[0;31m         return FastLlamaModel.from_pretrained(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m             \u001b[0;31m# Load vLLM first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1827\u001b[0;31m             \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mload_vllm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m             \u001b[0;31m# Convert to HF format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/vllm_utils.py\u001b[0m in \u001b[0;36mload_vllm\u001b[0;34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, return_args)\u001b[0m\n\u001b[1;32m   1162\u001b[0m                 )\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Model architectures ['Qwen2ForCausalLM'] failed to be inspected. Please check the logs for more details."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRO9ndqo5ky4"
      },
      "outputs": [],
      "source": [
        "#@title Test Your Model!\n",
        "\n",
        "# Generate test inputs\n",
        "print(\"Generating test inputs...\")\n",
        "test_inputs = await generate_training_inputs(TASK_DESCRIPTION, num_examples=NUM_TEST_INPUTS)\n",
        "\n",
        "print(f\"\\nðŸ§ª Testing the trained model on {len(test_inputs)} new inputs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, test_input in enumerate(test_inputs):\n",
        "    print(f\"\\nTest {i+1}:\")\n",
        "    print(f\"Input: {test_input}\")\n",
        "\n",
        "    # Run the model\n",
        "    test_task_input = TaskInput(\n",
        "        step=999,\n",
        "        input_text=test_input\n",
        "    )\n",
        "    result_trajectory = await rollout(model, test_task_input)\n",
        "\n",
        "    # Extract the model's response\n",
        "    messages = result_trajectory.messages()\n",
        "    model_response = messages[-1]['content'] if messages else \"No response\"\n",
        "\n",
        "    print(f\"Model output: {model_response}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Testing completed!\")\n",
        "print(f\"\\nYour model '{MODEL_NAME}' has been trained to: {TASK_DESCRIPTION}\")\n",
        "print(\"\\nTo use this model in production:\")\n",
        "print(\"1. The model checkpoint is saved in ./.art/\")\n",
        "print(\"2. You can load it using the vLLM library\")\n",
        "print(\"3. Or continue training with more examples by adjusting the configuration at the top\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a custom model for your task using only:\n",
        "- A task description\n",
        "- Example inputs (no outputs needed!)\n",
        "- RULER's automatic evaluation\n",
        "\n",
        "Here are some ways to improve results:\n",
        "\n",
        "1. **More diverse inputs**: Generate more varied input examples\n",
        "2. **Longer training**: Increase the number of training steps\n",
        "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
        "4. **Task refinement**: Make your task description more specific and detailed\n",
        "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
        "\n",
        "Remember: RULER learns what \"good\" means from your task description alone - no labeled data required!\n",
        "\n",
        "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai)."
      ],
      "metadata": {
        "id": "FuevYgXT-I1h"
      }
    }
  ]
}