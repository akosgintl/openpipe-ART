{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OpenPipe/ART/blob/auto-art/clean_auto_art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZYLROd8xnV"
      },
      "source": [
        "To train a model for your custom task, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**Custom Task Training with ART**\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 7B model to perform any single-turn task you describe - no labeled data needed! Simply describe what you want the model to learn, and this notebook will:\n",
        "\n",
        "1. Generate diverse input examples for your task\n",
        "2. Create an appropriate system prompt\n",
        "3. Train the model using RULER's automatic evaluation\n",
        "4. Test the trained model on new inputs\n",
        "\n",
        "RULER learns what makes a good output purely from your task description - no expected outputs required!\n",
        "\n",
        "You will learn how to use RULER for unsupervised learning, define custom [rollouts](https://art.openpipe.ai/resources/glossary#rollout), and run a [training loop](https://art.openpipe.ai/fundamentals/training-loop) that automatically improves your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "OsrwCDQ5cviC"
      },
      "outputs": [],
      "source": [
        "#@title 💿 Installation\n",
        "\n",
        "!uv pip install -q openpipe-art==0.3.11.post2 langchain-core tenacity \"gql<4\" --prerelease allow --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b8kgQ69ZDM"
      },
      "source": [
        "<a name=\"Configuration\"></a>\n",
        "\n",
        "### 🎯 Configuration - Edit These Settings\n",
        "\n",
        "Add an OpenRouter key and customize your training by modifying the values below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "so6r1_OG9en3"
      },
      "outputs": [],
      "source": [
        "# Required - Used for generating training inputs and RULER evaluation\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "\n",
        "# Optional - Enables metric logging\n",
        "WANDB_API_KEY = \"\"\n",
        "\n",
        "# Describe your custom task (be specific!)\n",
        "TASK_DESCRIPTION = \"\"\"\n",
        "Convert informal bug reports into structured JIRA-style tickets with these exact sections:\n",
        "- SUMMARY: (one line title)\n",
        "- PRIORITY: (Critical/High/Medium/Low based on impact)\n",
        "- STEPS TO REPRODUCE: (numbered list)\n",
        "- EXPECTED RESULT: (what should happen)\n",
        "- ACTUAL RESULT: (what actually happens)\n",
        "- ENVIRONMENT: (extracted system/version info)\n",
        "\"\"\"\n",
        "\n",
        "# Choose the base model to train\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Options: \"Qwen/Qwen2.5-1.5B-Instruct\", \"Qwen/Qwen2.5-3B-Instruct\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "I_AFDSOv_LrB"
      },
      "outputs": [],
      "source": [
        "#@title Advanced Settings\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"custom-task-model-001\"  # Name for your trained model\n",
        "PROJECT_NAME = \"custom-task-training\"  # Project name for tracking\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_training_inputs\": 25,  # Number of training inputs to generate\n",
        "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
        "    \"num_epochs\": 1,  # Number of times through all data\n",
        "    \"rollouts_per_group\": 3,  # Different responses per input (for RULER comparison)\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"max_training_steps\": None,  # Maximum training steps (set to None for no limit)\n",
        "}\n",
        "\n",
        "# Evaluation configuration\n",
        "RULER_MODEL = \"openrouter/deepseek/deepseek-r1-0528\"  # Model for RULER evaluation\n",
        "SYSTEM_PROMPT_GENERATION_MODEL=\"openrouter/moonshotai/kimi-k2\"\n",
        "INPUT_GENERATION_MODEL=\"openrouter/moonshotai/kimi-k2\"\n",
        "NUM_TEST_INPUTS = 5  # Number of test inputs to generate\n",
        "\n",
        "# GPU configuration (for T4 — keep these as-is unless you have a reason to change them)\n",
        "MAX_SEQ_LENGTH = 4096  # Maximum sequence length\n",
        "GPU_MEMORY_UTILIZATION = 0.7  # GPU memory usage (0.0-1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FQ2BfxTUKvgw",
        "outputId": "4ed46a50-3be1-4510-debb-af0a469b0128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-29 03:20:21 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 07-29 03:20:21 [__init__.py:239] Automatically detected platform cuda.\n",
            "Generating 25 training inputs...\n",
            "\n",
            "Generated 25 training inputs!\n",
            "\n",
            "First 5 examples:\n",
            "\n",
            "Example 1: Hey guys, just noticed that nothing happens when i try to sign in using my google account from the checkout page on safari. I click the big blue 'Continue with Google' button, but the popup doesn’t open at all. Any ideas? iOS 17.2, latest MacBook Pro.\n",
            "\n",
            "Example 2: Pls fix ASAP—enter any gift card longer than 12 digits and the whole iOS app hard-crashes. Every user on TestFlight build 8.12.3 hits the same thing (just send a 13-digit GC like 9999999999999 and boom). Had 500+ crash logs overnight. Running iPhone 14 Pro.\n",
            "\n",
            "Example 3: Super weird—if my email address contains a plus sign (+), the password-reset email never arrives. I’m on joe+test@mydomain.com, nothing ever shows up, yet it works fine when I do joe@test.com. Gmail web client in Chrome 123.\n",
            "\n",
            "Example 4: We’re on React-Native 0.72.4, Android 14, Samsung S24. Some emojis (🪩, 🫠, 🤌, 🫶) render as blank rectangles on the new comment screen. They look OK on web so maybe it’s just the app’s font stack?\n",
            "\n",
            "Example 5: Just deployed the ‘dark mode’ toggle. Turning it OFF while a file is uploading turns the progress bar white-on-white—nothing visible anymore. Edge 121 on Windows 11, company portal build 7.4.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/sky_workdir/.venv/lib/python3.10/site-packages/art/local/state.py:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  import unsloth  # type: ignore\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.5.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
            "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.47%\n",
            "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.1 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 368.\n",
            "Unsloth: vLLM's KV Cache can use up to 56.2 GB. Also swap space = 6 GB.\n",
            "INFO 07-29 03:22:20 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
            "WARNING 07-29 03:22:20 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 07-29 03:22:20 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
            "INFO 07-29 03:22:22 [cuda.py:292] Using Flash Attention backend.\n",
            "INFO 07-29 03:22:22 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 07-29 03:22:22 [model_runner.py:1108] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
            "INFO 07-29 03:22:22 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
            "INFO 07-29 03:22:23 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a69ee8df0ad40a9a322c171d8895704",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe8dc462ce32497084f930e905aa1517",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-29 03:22:27 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 07-29 03:22:28 [model_runner.py:1140] Model loading took 6.7340 GiB and 5.049031 seconds\n",
            "INFO 07-29 03:22:32 [worker.py:287] Memory profiling takes 3.29 seconds\n",
            "INFO 07-29 03:22:32 [worker.py:287] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.70) = 55.37GiB\n",
            "INFO 07-29 03:22:32 [worker.py:287] model weights take 6.73GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 2.04GiB; the rest of the memory reserved for KV Cache is 46.45GiB.\n",
            "INFO 07-29 03:22:32 [executor_base.py:112] # cuda blocks: 54363, # CPU blocks: 7021\n",
            "INFO 07-29 03:22:32 [executor_base.py:117] Maximum concurrency for 4096 tokens per request: 212.36x\n",
            "INFO 07-29 03:22:37 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 8.84 seconds\n",
            "Unsloth: Just some info: will skip parsing ['k_norm', 'q_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
            "Unsloth: Just some info: will skip parsing ['k_norm', 'q_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.5.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "Base model: Qwen/Qwen2.5-7B-Instruct\n",
            "Model name: custom-task-model-001\n",
            "Project name: custom-task-training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: weave version 0.51.59 is available!  To upgrade, please run:\n",
            "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install weave --upgrade\n",
            "INFO:weave.trace.init_message:weave version 0.51.59 is available!  To upgrade, please run:\n",
            " $ pip install weave --upgrade\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: openpipe.\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/openpipe-team/custom-task-training/weave\n",
            "INFO:weave.trace.init_message:Logged in as Weights & Biases user: openpipe.\n",
            "View Weave data at https://wandb.ai/openpipe-team/custom-task-training/weave\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated system prompt:\n",
            "\n",
            "You are a bug-ticket formatter.  \n",
            "For every user-supplied informal bug report, output a JIRA-style ticket that contains **only** the following six sections in the exact order and labels shown:\n",
            "\n",
            "SUMMARY: <one-line title of the issue>\n",
            "\n",
            "PRIORITY: <Critical|High|Medium|Low>  (choose based on user impact)\n",
            "\n",
            "STEPS TO REPRODUCE:  \n",
            "1. <first step>  \n",
            "2. <second step>  \n",
            "…\n",
            "\n",
            "EXPECTED RESULT: <what should happen>\n",
            "\n",
            "ACTUAL RESULT: <what actually happens>\n",
            "\n",
            "ENVIRONMENT: <extracted system, version, browser, OS, etc.>\n",
            "\n",
            "Do not add extra commentary, markdown, or sections.\n",
            "\n",
            "Rollout function defined!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\">[</span>RULER<span style=\"font-weight: bold\">]</span> Pretty-printed LLM choice JSON:\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m[\u001b[0mRULER\u001b[1m]\u001b[0m Pretty-printed LLM choice JSON:\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'scores'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"This trajectory successfully converts the informal request into formal business </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">language. It uses polite phrasing ('Could you please'), maintains professionalism ('at your earliest convenience'),</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and includes a proper closing ('Thank you'). It achieves the goal efficiently without unnecessary elements.\"</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.95</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"This response is somewhat formal but lacks the full professionalism expected in </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">business contexts. 'Can you' is acceptable but less polite than 'Could you', and 'soon' is vague. 'Thanks' is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">casual. It achieves the conversion goal partially but could be improved for higher formality.\"</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"This response fails to convert to formal language, retaining informal elements ('hey', </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'quick', 'thx'). It does not meet the goal of business formality and is actually more casual than the original </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">request in some aspects. No credit for goal achievement.\"</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'scores'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m\"This trajectory successfully converts the informal request into formal business \u001b[0m\n",
              "\u001b[32mlanguage. It uses polite phrasing \u001b[0m\u001b[32m(\u001b[0m\u001b[32m'Could you please'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, maintains professionalism \u001b[0m\u001b[32m(\u001b[0m\u001b[32m'at your earliest convenience'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\n",
              "\u001b[32mand includes a proper closing \u001b[0m\u001b[32m(\u001b[0m\u001b[32m'Thank you'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It achieves the goal efficiently without unnecessary elements.\"\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.95\u001b[0m,\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'1'\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m\"This response is somewhat formal but lacks the full professionalism expected in \u001b[0m\n",
              "\u001b[32mbusiness contexts. 'Can you' is acceptable but less polite than 'Could you', and 'soon' is vague. 'Thanks' is \u001b[0m\n",
              "\u001b[32mcasual. It achieves the conversion goal partially but could be improved for higher formality.\"\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.7\u001b[0m,\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'2'\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m\"This response fails to convert to formal language, retaining informal elements \u001b[0m\u001b[32m(\u001b[0m\u001b[32m'hey', \u001b[0m\n",
              "\u001b[32m'quick', 'thx'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It does not meet the goal of business formality and is actually more casual than the original \u001b[0m\n",
              "\u001b[32mrequest in some aspects. No credit for goal achievement.\"\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'3'\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Rank 1: Score 0.950\n",
            "  Response: Could you please send me the report at your earliest convenience? Thank you.\n",
            "\n",
            "Rank 2: Score 0.700\n",
            "  Response: Can you send me the report soon? Thanks.\n",
            "\n",
            "Rank 3: Score 0.100\n",
            "  Response: hey send report quick thx\n",
            "Starting training with 25 inputs...\n",
            "Training for 1 epoch(s)\n",
            "Generating 3 responses per input for RULER to compare\n",
            "\n",
            "Why multiple responses? RULER needs to compare different attempts to learn what's good!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47d6eee4966f4f6687c437a7310dc4c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iterating dataset:  62%|######1   | 8/13 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training step 8, epoch 0, epoch step 8\n",
            "Batch contains 2 inputs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "338d9895bbc74f43a03ab39f0e2f991e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating responses:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted checkpoint ./.art/custom-task-training/models/custom-task-model-001/0007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mopenpipe\u001b[0m (\u001b[33mopenpipe-team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/root/sky_workdir/wandb/run-20250729_032408-custom-task-model-001</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/openpipe-team/custom-task-training/runs/custom-task-model-001' target=\"_blank\">custom-task-model-001</a></strong> to <a href='https://wandb.ai/openpipe-team/custom-task-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/openpipe-team/custom-task-training' target=\"_blank\">https://wandb.ai/openpipe-team/custom-task-training</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/openpipe-team/custom-task-training/runs/custom-task-model-001' target=\"_blank\">https://wandb.ai/openpipe-team/custom-task-training/runs/custom-task-model-001</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wandb run initialized! You can view it at https://wandb.ai/openpipe-team/custom-task-training/runs/custom-task-model-001\n",
            "Packed 6 trajectories into 1 sequences of length 2048\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce67a77bd934466cb184532890bb2de8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 10,000,000 | Num Epochs = 3 | Total steps = 30,000,000\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 20,185,088/7,000,000,000 (0.29% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "Completed training step 8\n",
            "\n",
            "Training step 9, epoch 0, epoch step 9\n",
            "Batch contains 2 inputs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceaa98e24e564b33ab1a2bfb22b7db98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating responses:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted checkpoint ./.art/custom-task-training/models/custom-task-model-001/0008\n",
            "Packed 6 trajectories into 1 sequences of length 2048\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c871a833e34e460980c62b6030b5a00f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed training step 9\n",
            "\n",
            "Training step 10, epoch 0, epoch step 10\n",
            "Batch contains 2 inputs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d3ec02fb0964dccba17f6ece5645308",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating responses:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted checkpoint ./.art/custom-task-training/models/custom-task-model-001/0009\n",
            "Packed 3 trajectories into 1 sequences of length 2048\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80d3a85c436f4d519212fd181db3f8c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed training step 10\n",
            "\n",
            "Training step 11, epoch 0, epoch step 11\n",
            "Batch contains 2 inputs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cd01ffc48554b85984b1afacc99b3d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating responses:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted checkpoint ./.art/custom-task-training/models/custom-task-model-001/0010\n",
            "Packed 3 trajectories into 1 sequences of length 2048\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb5771b4e5c64f3a829b507364bab9bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed training step 11\n",
            "\n",
            "Training step 12, epoch 0, epoch step 12\n",
            "Batch contains 1 inputs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a31a4f921da841e29ab4e6021b89edb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating responses:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted checkpoint ./.art/custom-task-training/models/custom-task-model-001/0011\n",
            "Packed 3 trajectories into 1 sequences of length 2048\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2a7618e87bc465e852a166a0cb69f80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed training step 12\n",
            "\n",
            "✅ Training completed!\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to train your model!\n",
        "import art\n",
        "from art.local import LocalBackend\n",
        "from art.rewards import ruler_score_group\n",
        "from art.utils.litellm import convert_litellm_choice_to_openai\n",
        "from art.utils import iterate_dataset\n",
        "\n",
        "import weave\n",
        "from typing import List\n",
        "import os\n",
        "import random\n",
        "from pydantic import BaseModel, Field\n",
        "from litellm import acompletion\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Required\n",
        "if OPENROUTER_API_KEY:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n",
        "\n",
        "\n",
        "class TrainingInput(BaseModel):\n",
        "    input: str = Field(description=\"The input text for the task\")\n",
        "\n",
        "class TrainingDataset(BaseModel):\n",
        "    inputs: List[TrainingInput] = Field(description=\"List of training inputs\")\n",
        "\n",
        "async def generate_training_inputs(task_description: str, num_examples: int = 50) -> List[str]:\n",
        "    \"\"\"Generate diverse training inputs for the given task\"\"\"\n",
        "\n",
        "    system_prompt = f\"\"\"You are a helpful assistant that generates diverse, high-quality training inputs.\n",
        "\n",
        "Task: {task_description}\n",
        "\n",
        "Generate {num_examples} diverse INPUT examples that someone might provide for this task.\n",
        "Make sure the inputs:\n",
        "1. Cover a wide range of cases and edge cases\n",
        "2. Are realistic and practical\n",
        "3. Vary in length and complexity\n",
        "4. Represent real-world scenarios\n",
        "\n",
        "Only generate the INPUTS, not the outputs. RULER will evaluate the model's attempts automatically.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Generate {num_examples} input examples for the task described above. Return them in the form of a list.\"}\n",
        "    ]\n",
        "\n",
        "    print(f\"Generating {num_examples} training inputs...\")\n",
        "\n",
        "    inputs = []\n",
        "\n",
        "    i = 0\n",
        "    while i < 5 and len(inputs) < num_examples:\n",
        "      i += 1\n",
        "      response = await acompletion(\n",
        "          model=INPUT_GENERATION_MODEL,\n",
        "          messages=messages,\n",
        "          response_format=TrainingDataset,\n",
        "          temperature=1.0,\n",
        "      )\n",
        "\n",
        "      dataset = TrainingDataset.model_validate_json(response.choices[0].message.content)\n",
        "      inputs = [ex.input for ex in dataset.inputs]\n",
        "\n",
        "\n",
        "    if len(inputs) < num_examples:\n",
        "      raise ValueError(f\"Failed to generate {num_examples} training inputs.\")\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Generate training inputs\n",
        "training_inputs = await generate_training_inputs(TASK_DESCRIPTION, num_examples=TRAINING_CONFIG[\"num_training_inputs\"])\n",
        "print(f\"\\nGenerated {len(training_inputs)} training inputs!\")\n",
        "print(\"\\nFirst 5 examples:\")\n",
        "for i, input_text in enumerate(training_inputs[:5]):\n",
        "    print(f\"\\nExample {i+1}: {input_text}\")\n",
        "\n",
        "# =========== Model Creation Code ===========\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_NAME,\n",
        "    project=PROJECT_NAME,\n",
        "    base_model=BASE_MODEL,\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Base model:\", BASE_MODEL)\n",
        "print(\"Model name:\", MODEL_NAME)\n",
        "print(\"Project name:\", PROJECT_NAME)\n",
        "\n",
        "# ============ Rollout Function Code =============\n",
        "\n",
        "\n",
        "\n",
        "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
        "    weave.init(PROJECT_NAME, settings={\"print_call_link\": False})\n",
        "\n",
        "# Generate a system prompt for the task\n",
        "async def generate_system_prompt(task_description: str) -> str:\n",
        "    \"\"\"Generate an appropriate system prompt for the task\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Generate a clear, concise system prompt for a model that will perform the following task. The prompt should be direct and instructional.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Task: {task_description}\\n\\nGenerate a system prompt for this task.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=SYSTEM_PROMPT_GENERATION_MODEL,\n",
        "        messages=messages,\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "SYSTEM_PROMPT = await generate_system_prompt(TASK_DESCRIPTION)\n",
        "print(f\"Generated system prompt:\\n\\n{SYSTEM_PROMPT}\")\n",
        "\n",
        "class TaskInput(BaseModel):\n",
        "    step: int\n",
        "    input_text: str\n",
        "\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, task_input: TaskInput) -> art.Trajectory:\n",
        "    \"\"\"Execute a single rollout for the custom task\"\"\"\n",
        "\n",
        "    traj = art.Trajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"step\": task_input.step,\n",
        "            \"input\": task_input.input_text,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Build the conversation\n",
        "    traj.messages_and_choices = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": task_input.input_text},\n",
        "    ]\n",
        "\n",
        "    # Get model response\n",
        "    if model.trainable:\n",
        "        litellm_model_name = f\"hosted_vllm/{model.name}\"\n",
        "    else:\n",
        "        litellm_model_name = model.name\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=litellm_model_name,\n",
        "        base_url=model.inference_base_url,\n",
        "        api_key=model.inference_api_key,\n",
        "        temperature=0.7,\n",
        "        messages=traj.messages(),\n",
        "        caching=False,\n",
        "    )\n",
        "\n",
        "    # Add the model's response to the trajectory\n",
        "    traj.messages_and_choices.append(\n",
        "        convert_litellm_choice_to_openai(response.choices[0])\n",
        "    )\n",
        "\n",
        "    return traj\n",
        "\n",
        "print(\"\\nRollout function defined!\")\n",
        "\n",
        "\n",
        "# Test RULER with example outputs for a text formalization task\n",
        "test_input = \"hey can u send me the report asap? thx\"\n",
        "\n",
        "base_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Convert informal text to formal business language.\"},\n",
        "    {\"role\": \"user\", \"content\": test_input},\n",
        "]\n",
        "\n",
        "good_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Could you please send me the report at your earliest convenience? Thank you.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "mediocre_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Can you send me the report soon? Thanks.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "bad_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"hey send report quick thx\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "sample_group = art.TrajectoryGroup(\n",
        "    trajectories=[good_trajectory, mediocre_trajectory, bad_trajectory]\n",
        ")\n",
        "\n",
        "# RULER will score these based on how well they accomplish the task\n",
        "# three retries in case of API rate limiting\n",
        "for i in range(3):\n",
        "    try:\n",
        "        judged_group = await ruler_score_group(sample_group, RULER_MODEL, debug=True)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Error scoring group: {e}\")\n",
        "        continue\n",
        "\n",
        "assert judged_group is not None\n",
        "\n",
        "# Display rankings\n",
        "sorted_trajectories = sorted(\n",
        "    judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        ")\n",
        "for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "    messages = traj.messages()\n",
        "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "    print(f\"  Response: {messages[-1]['content']}\")\n",
        "\n",
        "\n",
        "# ============ Training Loop =============\n",
        "\n",
        "# Convert training inputs to TaskInput objects\n",
        "training_task_inputs = [\n",
        "    TaskInput(step=0, input_text=inp)\n",
        "    for inp in training_inputs\n",
        "]\n",
        "\n",
        "# Create training iterator\n",
        "training_iterator = iterate_dataset(\n",
        "    training_task_inputs,\n",
        "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
        "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "\n",
        "print(f\"Starting training with {len(training_task_inputs)} inputs...\")\n",
        "print(f\"Training for {TRAINING_CONFIG['num_epochs']} epoch(s)\")\n",
        "print(f\"Generating {TRAINING_CONFIG['rollouts_per_group']} responses per input for RULER to compare\")\n",
        "print(f\"\\nWhy multiple responses? RULER needs to compare different attempts to learn what's good!\")\n",
        "\n",
        "for batch, epoch, global_step, epoch_step in training_iterator:\n",
        "    print(f\"\\nTraining step {global_step}, epoch {epoch}, epoch step {epoch_step}\")\n",
        "    print(f\"Batch contains {len(batch)} inputs\")\n",
        "\n",
        "    # Create trajectory groups for this batch\n",
        "    groups = []\n",
        "    for task_input in batch:\n",
        "        # Update step number\n",
        "        task_input.step = global_step\n",
        "\n",
        "        # Generate multiple responses for each input (RULER will compare these)\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    rollout(model, task_input)\n",
        "                    for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"Generating responses\",\n",
        "        max_exceptions=TRAINING_CONFIG[\"rollouts_per_group\"] * len(batch),\n",
        "    )\n",
        "\n",
        "    # Use RULER to score each group\n",
        "    judged_groups = []\n",
        "    for group in finished_groups:\n",
        "        judged_group = await ruler_score_group(\n",
        "            group,\n",
        "            RULER_MODEL,\n",
        "            debug=False\n",
        "        )\n",
        "        judged_groups.append(judged_group)\n",
        "\n",
        "    # Train on the scored trajectories\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        judged_groups,\n",
        "        config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]),\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )\n",
        "\n",
        "    print(f\"Completed training step {global_step}\")\n",
        "\n",
        "    # Stop after configured steps (if limit is set)\n",
        "    if TRAINING_CONFIG[\"max_training_steps\"] and global_step >= TRAINING_CONFIG[\"max_training_steps\"]:\n",
        "        print(f\"Reached maximum training steps ({TRAINING_CONFIG['max_training_steps']})\")\n",
        "        break\n",
        "\n",
        "print(\"\\n✅ Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "YRO9ndqo5ky4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating test inputs...\n",
            "Generating 5 training inputs...\n",
            "\n",
            "🧪 Testing the trained model on 5 new inputs:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Test 1:\n",
            "Input: yo, so every time I open the iOS app on my 12 pro max running 15.6 and try to upload a pic that’s any bigger than like maybe 2 mb, the whole thing just fucking dies and goes back to the home screen. it’s like the image is radioactive lmfao. smaller images go through fine so it’s obviously the file size but cmon this ain’t 2003. help or ima rage uninstall thx\n",
            "Model output: SUMMARY: App crashes when uploading images larger than 2MB on iOS\n",
            "\n",
            "PRIORITY: Medium\n",
            "\n",
            "STEPS TO REPRODUCE:  \n",
            "1. Open the iOS app on iPhone 12 Pro Max running iOS 15.6  \n",
            "2. Attempt to upload an image larger than 2MB  \n",
            "\n",
            "EXPECTED RESULT: Image uploads successfully\n",
            "\n",
            "ACTUAL RESULT: App crashes and returns to the home screen\n",
            "\n",
            "ENVIRONMENT: iOS 15.6, iPhone 12 Pro Max\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 2:\n",
            "Input: Bug spotted: CSV button on the analytics dashboard (chrome, pc, latest) turns green after click but no file downloads (Win11 22H2, 1440p screen). Same thing last Friday and now again—worked perfectly before 2 PM CET. No errors in console, cookies cleared. We need that report by Tuesday for the client.\n",
            "Model output: SUMMARY: CSV button on analytics dashboard does not download file\n",
            "\n",
            "PRIORITY: High\n",
            "\n",
            "STEPS TO REPRODUCE:  \n",
            "1. Navigate to the analytics dashboard  \n",
            "2. Click on the CSV button  \n",
            "\n",
            "EXPECTED RESULT: A CSV file should start downloading  \n",
            "\n",
            "ACTUAL RESULT: The button turns green but no file downloads  \n",
            "\n",
            "ENVIRONMENT: Chrome (latest), Windows 11 22H2, 1440p screen\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 3:\n",
            "Input: Priority critical—the login server throws 502 errors on first attempt (EU region only). Repro: 1. open login.example.com, 2. enter any creds, 3. hit sign in: instant 502. Second try succeeds, so users can get in, but our bounce rate is through the roof. Prod env, Kubernetes v1.24 cluster, deployed this morning at 9:13 UTC.\n",
            "Model output: SUMMARY: Login server throws 502 errors on first attempt in EU region\n",
            "\n",
            "PRIORITY: Critical\n",
            "\n",
            "STEPS TO REPRODUCE:  \n",
            "1. open login.example.com  \n",
            "2. enter any creds  \n",
            "3. hit sign in  \n",
            "\n",
            "EXPECTED RESULT: Successful login\n",
            "\n",
            "ACTUAL RESULT: Instant 502 error\n",
            "\n",
            "ENVIRONMENT: Prod env, Kubernetes v1.24 cluster, deployed this morning at 9:13 UTC\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 4:\n",
            "Input: voiceover on ipad (ios 16.1) gets stuck reading the product description in the new ecommerce view—just loops “loading” over and over. happens with airpods pro and built-in speaker. if you turn VO off > on it resumes halfway down the list then stops again. no visible focus ring, feels like focus management is blown to pieces.\n",
            "Model output: SUMMARY: VoiceOver on iPad (iOS 16.1) gets stuck reading product description in new ecommerce view\n",
            "\n",
            "PRIORITY: High\n",
            "\n",
            "STEPS TO REPRODUCE:  \n",
            "1. Open the new ecommerce view on iPad (iOS 16.1)  \n",
            "2. Enable VoiceOver  \n",
            "3. Navigate to the product description  \n",
            "\n",
            "EXPECTED RESULT: VoiceOver should read through the product description without getting stuck  \n",
            "\n",
            "ACTUAL RESULT: VoiceOver gets stuck in a loop reading \"loading\"  \n",
            "\n",
            "ENVIRONMENT: iPad (iOS 16.1), AirPods Pro, Built-in Speaker\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 5:\n",
            "Input: quick one: dark mode toggle on android 13 (pixel 7) switches ui alright but webview pages keep glaring white even after restart until reinstall. beta channel build 3.2.0 (10230). easy to fix I guess but it’s hurting eyes at night lol\n",
            "Model output: SUMMARY: Dark mode toggle on Android 13 (Pixel 7) fails to change webview pages to dark until app reinstall\n",
            "\n",
            "PRIORITY: Low\n",
            "\n",
            "STEPS TO REPRODUCE:  \n",
            "1. Enable dark mode on app settings on Android 13 (Pixel 7)  \n",
            "2. Open any webview page  \n",
            "3. The webview page remains glaring white  \n",
            "\n",
            "EXPECTED RESULT: Webview pages should change to dark mode when dark mode is toggled on the app  \n",
            "\n",
            "ACTUAL RESULT: Webview pages remain white even after enabling dark mode and restarting the app  \n",
            "\n",
            "ENVIRONMENT: Beta channel build 3.2.0 (10230) on Android 13 (Pixel 7)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🎉 Testing completed!\n",
            "\n",
            "Your model 'custom-task-model-001' has been trained to: \n",
            "Convert informal bug reports into structured JIRA-style tickets with these exact sections:\n",
            "- SUMMARY: (one line title)\n",
            "- PRIORITY: (Critical/High/Medium/Low based on impact)\n",
            "- STEPS TO REPRODUCE: (numbered list)\n",
            "- EXPECTED RESULT: (what should happen)\n",
            "- ACTUAL RESULT: (what actually happens)\n",
            "- ENVIRONMENT: (extracted system/version info)\n",
            "\n",
            "\n",
            "To use this model in production:\n",
            "1. The model checkpoint is saved in ./.art/\n",
            "2. You can load it using the vLLM library\n",
            "3. Or continue training with more examples by adjusting the configuration at the top\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/01985439-f8c8-7be2-b43b-470687306a2d\n",
            "INFO:weave.trace.op:🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/01985439-f8c8-7be2-b43b-470687306a2d\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/0198543a-3b56-7430-bc6a-8575e6aba080\n",
            "INFO:weave.trace.op:🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/0198543a-3b56-7430-bc6a-8575e6aba080\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/0198543a-73df-7817-966f-802713d30304\n",
            "INFO:weave.trace.op:🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/0198543a-73df-7817-966f-802713d30304\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/0198543a-ae28-7113-9cac-49d72178b808\n",
            "INFO:weave.trace.op:🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/0198543a-ae28-7113-9cac-49d72178b808\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/0198543a-fd1d-70fa-8bba-bf7412af0d40\n",
            "INFO:weave.trace.op:🍩 https://wandb.ai/openpipe-team/custom-task-training/r/call/0198543a-fd1d-70fa-8bba-bf7412af0d40\n"
          ]
        }
      ],
      "source": [
        "#@title Test Your Model!\n",
        "\n",
        "# Generate test inputs\n",
        "print(\"Generating test inputs...\")\n",
        "test_inputs = await generate_training_inputs(TASK_DESCRIPTION, num_examples=NUM_TEST_INPUTS)\n",
        "\n",
        "print(f\"\\n🧪 Testing the trained model on {len(test_inputs)} new inputs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, test_input in enumerate(test_inputs):\n",
        "    print(f\"\\nTest {i+1}:\")\n",
        "    print(f\"Input: {test_input}\")\n",
        "\n",
        "    # Run the model\n",
        "    test_task_input = TaskInput(\n",
        "        step=999,\n",
        "        input_text=test_input\n",
        "    )\n",
        "    result_trajectory = await rollout(model, test_task_input)\n",
        "\n",
        "    # Extract the model's response\n",
        "    messages = result_trajectory.messages()\n",
        "    model_response = messages[-1]['content'] if messages else \"No response\"\n",
        "\n",
        "    print(f\"Model output: {model_response}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n🎉 Testing completed!\")\n",
        "print(f\"\\nYour model '{MODEL_NAME}' has been trained to: {TASK_DESCRIPTION}\")\n",
        "print(\"\\nTo use this model in production:\")\n",
        "print(\"1. The model checkpoint is saved in ./.art/\")\n",
        "print(\"2. You can load it using the vLLM library\")\n",
        "print(\"3. Or continue training with more examples by adjusting the configuration at the top\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuevYgXT-I1h"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a custom model for your task using only:\n",
        "- A task description\n",
        "- Example inputs (no outputs needed!)\n",
        "- RULER's automatic evaluation\n",
        "\n",
        "Here are some ways to improve results:\n",
        "\n",
        "1. **More diverse inputs**: Generate more varied input examples\n",
        "2. **Longer training**: Increase the number of training steps\n",
        "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
        "4. **Task refinement**: Make your task description more specific and detailed\n",
        "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
        "\n",
        "Remember: RULER learns what \"good\" means from your task description alone - no labeled data required!\n",
        "\n",
        "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
