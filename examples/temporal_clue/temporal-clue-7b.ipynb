{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sky_workdir/examples/temporal_clue/wandb/run-20250716_171107-005</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bradhilton/temporal-clue/runs/005' target=\"_blank\">005</a></strong> to <a href='https://wandb.ai/bradhilton/temporal-clue' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/temporal-clue' target=\"_blank\">https://wandb.ai/bradhilton/temporal-clue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/temporal-clue/runs/005' target=\"_blank\">https://wandb.ai/bradhilton/temporal-clue/runs/005</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-16 17:11:20 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sky_workdir/src/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore # noqa: F401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "WARNING 07-16 17:11:39 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 07-16 17:11:39 [__init__.py:244] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.6.12: Fast Qwen2 patching. Transformers: 4.53.2. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.47%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.1 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 368.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.2 GB. Also swap space = 6 GB.\n",
      "INFO 07-16 17:12:07 [config.py:823] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 07-16 17:12:07 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":368,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-16 17:12:11 [cuda.py:327] Using Flash Attention backend.\n",
      "INFO 07-16 17:12:12 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-16 17:12:12 [model_runner.py:1171] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
      "INFO 07-16 17:12:13 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 07-16 17:12:13 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from typing import TypedDict\n",
    "\n",
    "from art.local import LocalBackend\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class TemporalCluePuzzle(TypedDict):\n",
    "    num_clues: int\n",
    "    prompt: str\n",
    "    solution: dict[str, str]\n",
    "\n",
    "\n",
    "puzzles_path = \"../data/temporal-clue/puzzles.json\"\n",
    "puzzles: list[TemporalCluePuzzle] = json.loads(open(puzzles_path).read())\n",
    "val_puzzles = puzzles[:64]\n",
    "test_puzzles = puzzles[64:128]\n",
    "train_puzzles = puzzles[128:]\n",
    "random.seed(42)\n",
    "random.shuffle(train_puzzles)\n",
    "\n",
    "\n",
    "async def rollout(model: art.Model, puzzle: TemporalCluePuzzle) -> art.Trajectory:\n",
    "    messages: art.Messages = [{\"role\": \"user\", \"content\": puzzle[\"prompt\"] + \" /no_think\"}]\n",
    "    client = model.openai_client()\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=messages, model=model.name\n",
    "    )\n",
    "    choice = chat_completion.choices[0]\n",
    "    content = choice.message.content\n",
    "    assert isinstance(content, str)\n",
    "    num_correct = 0\n",
    "    for key, value in puzzle[\"solution\"].items():\n",
    "        if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "            match = matches[-1]\n",
    "            if match.strip().lower() == value.lower():\n",
    "                num_correct += 1\n",
    "    reward = acc = num_correct / len(puzzle[\"solution\"])\n",
    "    return art.Trajectory(\n",
    "        messages_and_choices=[*messages, choice], reward=reward, metrics={\"acc\": acc}\n",
    "    )\n",
    "\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=\"005\",\n",
    "    project=\"temporal-clue\",\n",
    "    base_model=\"Qwen/Qwen2.5-7B-Instruct\"\n",
    ")\n",
    "backend = LocalBackend()\n",
    "await model.register(backend)\n",
    "\n",
    "stride = 8\n",
    "for i in range(await model.get_step(), 1_000):\n",
    "    val_groups, train_groups = await asyncio.gather(\n",
    "        art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(rollout(model, puzzle) for _ in range(1))\n",
    "                for puzzle in val_puzzles\n",
    "            ),\n",
    "            pbar_desc=\"val\",\n",
    "            pbar_total_completion_tokens=False,\n",
    "        ),\n",
    "        art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(rollout(model, puzzle) for _ in range(32))\n",
    "                for puzzle in train_puzzles[i * stride : (i + 1) * stride]\n",
    "            ),\n",
    "            pbar_desc=\"train\",\n",
    "            pbar_total_completion_tokens=False,\n",
    "        ),\n",
    "    )\n",
    "    await model.log(val_groups)\n",
    "    await model.delete_checkpoints()\n",
    "    await model.train(\n",
    "        train_groups,\n",
    "        config=art.TrainConfig(learning_rate=1e-6),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
