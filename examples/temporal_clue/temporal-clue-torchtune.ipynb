{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/sky_workdir/examples/temporal_clue/wandb/run-20250710_214910-002</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/temporal-clue/runs/002' target=\"_blank\">002</a></strong> to <a href='https://wandb.ai/bradhilton/temporal-clue' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/temporal-clue' target=\"_blank\">https://wandb.ai/bradhilton/temporal-clue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/temporal-clue/runs/002' target=\"_blank\">https://wandb.ai/bradhilton/temporal-clue/runs/002</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 21:49:15 [__init__.py:244] Automatically detected platform cuda.\n",
      "WARNING 07-10 21:49:21 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 07-10 21:49:21 [__init__.py:244] Automatically detected platform cuda.\n",
      "/home/ubuntu/.cache/huggingface/hub/models--willcb--Qwen3-14B/snapshots/ad504088bc654f8e9e4f0af2461743db6877fa32\n",
      "INFO 07-10 21:49:30 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'generate', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-10 21:49:31 [config.py:1946] Defaulting to use mp for distributed inference\n",
      "INFO 07-10 21:49:31 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 07-10 21:49:31 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 07-10 21:49:32 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 07-10 21:49:34 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-10 21:49:36 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 07-10 21:49:36 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='willcb/Qwen3-14B', speculative_config=None, tokenizer='willcb/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=willcb/Qwen3-14B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-10 21:49:36 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 26 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-10 21:49:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_bc7d331b'), local_subscribe_addr='ipc:///tmp/e94356af-6342-40a8-94b8-a6a2da8e0dee', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-10 21:49:37 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "WARNING 07-10 21:49:37 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 07-10 21:49:39 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-10 21:49:39 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-10 21:49:42 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "WARNING 07-10 21:49:42 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d749f07aa40>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_94b0ef7a'), local_subscribe_addr='ipc:///tmp/904de432-2d70-4826-946a-b35d9e7775ab', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-10 21:49:42 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "WARNING 07-10 21:49:42 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x737b11e6a9e0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4ba441f6'), local_subscribe_addr='ipc:///tmp/39ed1e39-0ce7-41d4-a763-cbdfaebc1c08', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:43 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:43 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:43 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:43 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:43 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:43 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_3858a116'), local_subscribe_addr='ipc:///tmp/64588bfa-b42f-43b5-92eb-01b22acc5a4c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:43 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 07-10 21:49:43 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m WARNING 07-10 21:49:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 07-10 21:49:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:43 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-14B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:43 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-14B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:43 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:43 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:44 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:44 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:44 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:44 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:02,  1.68it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:02,  1.42it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:02<00:02,  1.38it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:02<00:01,  1.32it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:03<00:00,  1.38it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.38it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:48 [default_loader.py:272] Loading weights took 4.38 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:48 [default_loader.py:272] Loading weights took 4.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:49 [gpu_model_runner.py:1624] Model loading took 13.8818 GiB and 4.778461 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:49 [gpu_model_runner.py:1624] Model loading took 13.8818 GiB and 4.848332 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:57 [backends.py:462] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/b081e4db96/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:49:57 [backends.py:472] Dynamo bytecode transform time: 8.39 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:57 [backends.py:462] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/b081e4db96/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:49:57 [backends.py:472] Dynamo bytecode transform time: 8.52 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:50:04 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 6.035 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:50:04 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 6.049 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:50:06 [monitor.py:34] torch.compile takes 8.52 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:50:06 [monitor.py:34] torch.compile takes 8.39 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:50:06 [gpu_worker.py:227] Available KV cache memory: 37.18 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:50:06 [gpu_worker.py:227] Available KV cache memory: 37.18 GiB\n",
      "INFO 07-10 21:50:07 [kv_cache_utils.py:715] GPU KV cache size: 487,312 tokens\n",
      "INFO 07-10 21:50:07 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 11.90x\n",
      "INFO 07-10 21:50:07 [kv_cache_utils.py:715] GPU KV cache size: 487,312 tokens\n",
      "INFO 07-10 21:50:07 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 11.90x\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:50:30 [custom_all_reduce.py:196] Registering 5427 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:50:30 [custom_all_reduce.py:196] Registering 5427 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=71560)\u001b[0;0m INFO 07-10 21:50:30 [gpu_model_runner.py:2048] Graph capturing finished in 23 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=71557)\u001b[0;0m INFO 07-10 21:50:30 [gpu_model_runner.py:2048] Graph capturing finished in 23 secs, took 0.77 GiB\n",
      "INFO 07-10 21:50:30 [core.py:171] init engine (profile, create kv cache, warmup model) took 41.52 seconds\n",
      "INFO 07-10 21:50:31 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 30457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe5b69483ea466aa2402759223ddc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40144e3efb3c4641a8db733250cf6396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed 64 trajectories into 64 sequences of length 26624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2c5f7e39794e138934ee7fbba29a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Train process exited early. See /home/ubuntu/sky_workdir/.art/temporal-clue/models/002/logs/train.log for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlog(val_groups)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdelete_checkpoints()\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     93\u001b[0m     train_groups,\n\u001b[1;32m     94\u001b[0m     config\u001b[38;5;241m=\u001b[39mart\u001b[38;5;241m.\u001b[39mTrainConfig(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-6\u001b[39m),\n\u001b[1;32m     95\u001b[0m )\n",
      "File \u001b[0;32m~/sky_workdir/src/art/model.py:353\u001b[0m, in \u001b[0;36mTrainableModel.train\u001b[0;34m(self, trajectory_groups, config, _config, verbose)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    339\u001b[0m     trajectory_groups: Iterable[TrajectoryGroup],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    343\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    Reinforce fine-tune the model with a batch of trajectory groups.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m            not yet part of the public API. Use at your own risk.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend()\u001b[38;5;241m.\u001b[39m_train_model(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(trajectory_groups), config, _config \u001b[38;5;129;01mor\u001b[39;00m {}, verbose\n\u001b[1;32m    355\u001b[0m     ):\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/sky_workdir/src/art/local/backend.py:358\u001b[0m, in \u001b[0;36mLocalBackend._train_model\u001b[0;34m(self, model, trajectory_groups, config, dev_config, verbose)\u001b[0m\n\u001b[1;32m    356\u001b[0m     estimated_gradient_steps \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(estimated_gradient_steps \u001b[38;5;241m/\u001b[39m dp)\n\u001b[1;32m    357\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39mestimated_gradient_steps, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 358\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m service\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    359\u001b[0m     disk_packed_tensors, config, dev_config, verbose\n\u001b[1;32m    360\u001b[0m ):\n\u001b[1;32m    361\u001b[0m     num_gradient_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[1;32m    362\u001b[0m         result\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_gradient_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimated_gradient_steps)\n\u001b[1;32m    363\u001b[0m     )\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m num_gradient_steps \u001b[38;5;241m==\u001b[39m estimated_gradient_steps, (\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_gradient_steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_gradient_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != estimated_gradient_steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_gradient_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n",
      "File \u001b[0;32m~/sky_workdir/src/mp_actors/move.py:133\u001b[0m, in \u001b[0;36mProxy.__getattr__.<locals>.async_gen_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     send_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         send_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_response(\n\u001b[1;32m    134\u001b[0m             args, kwargs, \u001b[38;5;28mid\u001b[39m, send_value\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m (), {}\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/sky_workdir/src/mp_actors/move.py:119\u001b[0m, in \u001b[0;36mProxy.__getattr__.<locals>.get_response\u001b[0;34m(args, kwargs, id, send_value)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_futures[request\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mFuture()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requests\u001b[38;5;241m.\u001b[39mput_nowait(request)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_futures[request\u001b[38;5;241m.\u001b[39mid]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/asyncio/futures.py:285\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sky_workdir/src/mp_actors/move.py:235\u001b[0m, in \u001b[0;36m_handle_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generators:\n\u001b[1;32m    232\u001b[0m         generators[request\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m result_or_callable(\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;241m*\u001b[39mrequest\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m    234\u001b[0m         )\n\u001b[0;32m--> 235\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generators[request\u001b[38;5;241m.\u001b[39mid]\u001b[38;5;241m.\u001b[39masend(request\u001b[38;5;241m.\u001b[39msend_value)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(result_or_callable):\n\u001b[1;32m    237\u001b[0m     result_or_coro \u001b[38;5;241m=\u001b[39m result_or_callable(\u001b[38;5;241m*\u001b[39mrequest\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/sky_workdir/src/art/torchtune/service.py:107\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m result\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain process exited early. See \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/logs/train.log for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             )\n\u001b[1;32m    110\u001b[0m     num_gradient_steps \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# wait for the workers to wake up\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Train process exited early. See /home/ubuntu/sky_workdir/.art/temporal-clue/models/002/logs/train.log for details."
     ]
    }
   ],
   "source": [
    "import art\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from typing import TypedDict\n",
    "\n",
    "from art.local import LocalBackend\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class TemporalCluePuzzle(TypedDict):\n",
    "    num_clues: int\n",
    "    prompt: str\n",
    "    solution: dict[str, str]\n",
    "\n",
    "\n",
    "puzzles_path = \"../data/temporal-clue/puzzles.json\"\n",
    "puzzles: list[TemporalCluePuzzle] = json.loads(open(puzzles_path).read())\n",
    "val_puzzles = puzzles[:64]\n",
    "test_puzzles = puzzles[64:128]\n",
    "train_puzzles = puzzles[128:]\n",
    "random.seed(42)\n",
    "random.shuffle(train_puzzles)\n",
    "\n",
    "\n",
    "async def rollout(model: art.Model, puzzle: TemporalCluePuzzle) -> art.Trajectory:\n",
    "    messages: art.Messages = [{\"role\": \"user\", \"content\": puzzle[\"prompt\"]}]\n",
    "    client = model.openai_client()\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=messages, model=model.name\n",
    "    )\n",
    "    choice = chat_completion.choices[0]\n",
    "    content = choice.message.content\n",
    "    assert isinstance(content, str)\n",
    "    num_correct = 0\n",
    "    for key, value in puzzle[\"solution\"].items():\n",
    "        if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "            match = matches[-1]\n",
    "            if match.strip().lower() == value.lower():\n",
    "                num_correct += 1\n",
    "    reward = acc = num_correct / len(puzzle[\"solution\"])\n",
    "    return art.Trajectory(\n",
    "        messages_and_choices=[*messages, choice], reward=reward, metrics={\"acc\": acc}\n",
    "    )\n",
    "\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=\"002\",\n",
    "    project=\"temporal-clue\",\n",
    "    base_model=\"willcb/Qwen3-14B\",\n",
    "    _internal_config={\n",
    "        \"engine_args\": {\n",
    "            \"tensor_parallel_size\": 2,\n",
    "            \"gpu_memory_utilization\": 0.7,\n",
    "            \"max_num_seqs\": 512,\n",
    "        },\n",
    "        \"torchtune_args\": {\n",
    "            \"model\": \"qwen3_14b_instruct\",\n",
    "            \"model_type\": \"QWEN3\",\n",
    "            \"async_weight_syncing\": True,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "backend = LocalBackend()\n",
    "await model.register(backend)\n",
    "\n",
    "stride = 4\n",
    "for i in range(await model.get_step(), 1_000):\n",
    "    val_groups, train_groups = await asyncio.gather(\n",
    "        art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(rollout(model, puzzle) for _ in range(1))\n",
    "                for puzzle in val_puzzles\n",
    "            ),\n",
    "            pbar_desc=\"val\",\n",
    "            pbar_total_completion_tokens=False,\n",
    "        ),\n",
    "        art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(rollout(model, puzzle) for _ in range(16))\n",
    "                for puzzle in train_puzzles[i * stride : (i + 1) * stride]\n",
    "            ),\n",
    "            pbar_desc=\"train\",\n",
    "            pbar_total_completion_tokens=False,\n",
    "        ),\n",
    "    )\n",
    "    await model.log(val_groups)\n",
    "    await model.delete_checkpoints()\n",
    "    await model.train(\n",
    "        train_groups,\n",
    "        config=art.TrainConfig(learning_rate=5e-6),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
