{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ff6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f78de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/sky_workdir/dev/wandb/run-20250822_022145-test</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/tests/runs/test' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/bradhilton/tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/tests' target=\"_blank\">https://wandb.ai/bradhilton/tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/tests/runs/test' target=\"_blank\">https://wandb.ai/bradhilton/tests/runs/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-22 02:21:51 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/sky_workdir/src/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore # noqa: F401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 08-22 02:21:59 [__init__.py:235] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.53.2. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.47%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.19 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 368.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.27 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 08-22 02:22:18 [config.py:1604] Using max model len 32768\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 08-22 02:22:18 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":true,\"trace.graph_diagram\":false,\"compile_threads\":26,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":368,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 08-22 02:22:20 [cuda.py:398] Using Flash Attention backend.\n",
      "INFO 08-22 02:22:20 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-22 02:22:20 [model_runner.py:1083] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
      "INFO 08-22 02:22:21 [bitsandbytes_loader.py:733] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 08-22 02:22:21 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 25.05it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.53it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-22 02:22:23 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 08-22 02:22:24 [model_runner.py:1115] Model loading took 6.7355 GiB and 2.672879 seconds\n",
      "INFO 08-22 02:22:27 [worker.py:295] Memory profiling takes 2.94 seconds\n",
      "INFO 08-22 02:22:27 [worker.py:295] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.78) = 62.14GiB\n",
      "INFO 08-22 02:22:27 [worker.py:295] model weights take 6.74GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.72GiB; the rest of the memory reserved for KV Cache is 50.54GiB.\n",
      "INFO 08-22 02:22:27 [executor_base.py:113] # cuda blocks: 59142, # CPU blocks: 7021\n",
      "INFO 08-22 02:22:27 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 28.88x\n",
      "INFO 08-22 02:22:31 [vllm_utils.py:671] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 08-22 02:22:31 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:11<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-22 02:22:43 [model_runner.py:1537] Graph capturing finished in 12 secs, took 1.35 GiB\n",
      "INFO 08-22 02:22:43 [vllm_utils.py:678] Unsloth: Patched vLLM v0 graph capture finished in 12 secs.\n",
      "INFO 08-22 02:22:43 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 19.60 seconds\n",
      "Unsloth: Just some info: will skip parsing ['k_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm', 'q_norm']\n",
      "Unsloth: Just some info: will skip parsing ['k_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm', 'q_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from art.local import LocalBackend\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=\"test\",\n",
    "    project=\"tests\",\n",
    "    base_model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    _internal_config=art.dev.InternalModelConfig(\n",
    "        engine_args=art.dev.EngineArgs(\n",
    "            num_scheduler_steps=1,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "await model.register(LocalBackend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd4bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = model.openai_client()\n",
    "\n",
    "chat_completion = await client.chat.completions.create(\n",
    "    model=\"test\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hi!\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce12643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam\n",
    "from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam\n",
    "\n",
    "message: ChatCompletionMessageParam = {\"role\": \"user\", \"content\": \"Hi!\"}\n",
    "tools: list[ChatCompletionToolParam] = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": [],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_completion = await client.chat.completions.create(\n",
    "    model=\"test\",\n",
    "    messages=[message],\n",
    "    tool_choice=\"required\",\n",
    "    tools=tools,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7233efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:314\",\"logprob\":-0.00015293381875380874,\"bytes\":[32,123],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:314\",\"logprob\":-0.00015293381875380874,\"bytes\":[32,123],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":null,\"arguments\":\"}\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:3417\",\"logprob\":-3.576278118089249e-7,\"bytes\":[125,125],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":null,\"arguments\":\"}\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:3417\",\"logprob\":-3.576278118089249e-7,\"bytes\":[125,125],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: [DONE]\\n\\ndata: [DONE]\\n\\n'\n",
      "b'data: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:314\",\"logprob\":-0.00015293381875380874,\"bytes\":[32,123],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:314\",\"logprob\":-0.00015293381875380874,\"bytes\":[32,123],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":null,\"arguments\":\"}\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:3417\",\"logprob\":-3.576278118089249e-7,\"bytes\":[125,125],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":null,\"arguments\":\"}\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:3417\",\"logprob\":-3.576278118089249e-7,\"bytes\":[125,125],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: [DONE]\\n\\ndata: [DONE]\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:314\",\"logprob\":-0.00015293381875380874,\"bytes\":[32,123],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:314\",\"logprob\":-0.00015293381875380874,\"bytes\":[32,123],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":null,\"arguments\":\"}\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:3417\",\"logprob\":-3.576278118089249e-7,\"bytes\":[125,125],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":null,\"arguments\":\"}\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:3417\",\"logprob\":-3.576278118089249e-7,\"bytes\":[125,125],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: [DONE]\\n\\ndata: [DONE]\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:314\",\"logprob\":-0.00015293381875380874,\"bytes\":[32,123],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:314\",\"logprob\":-0.00015293381875380874,\"bytes\":[32,123],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":null,\"arguments\":\"}\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:3417\",\"logprob\":-3.576278118089249e-7,\"bytes\":[125,125],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: {\"id\":\"chatcmpl-aad436fb6a564c3da716f2b9feb970a4\",\"object\":\"chat.completion.chunk\",\"created\":1755831694,\"model\":\"test\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":null,\"arguments\":\"}\"}}]},\"logprobs\":{\"content\":[{\"token\":\"token_id:3417\",\"logprob\":-3.576278118089249e-7,\"bytes\":[125,125],\"top_logprobs\":[]}]},\"finish_reason\":null}]}\\n\\ndata: [DONE]\\n\\ndata: [DONE]\\n\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'stop',\n",
       " 'index': 0,\n",
       " 'logprobs': {'content': [{'token': 'token_id:314',\n",
       "    'bytes': [32, 123],\n",
       "    'logprob': -0.00015293381875380874,\n",
       "    'top_logprobs': []},\n",
       "   {'token': 'token_id:314',\n",
       "    'bytes': [32, 123],\n",
       "    'logprob': -0.00015293381875380874,\n",
       "    'top_logprobs': []},\n",
       "   {'token': 'token_id:3417',\n",
       "    'bytes': [125, 125],\n",
       "    'logprob': -3.576278118089249e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': 'token_id:3417',\n",
       "    'bytes': [125, 125],\n",
       "    'logprob': -3.576278118089249e-07,\n",
       "    'top_logprobs': []}],\n",
       "  'refusal': None},\n",
       " 'message': {'content': None,\n",
       "  'refusal': None,\n",
       "  'role': 'assistant',\n",
       "  'annotations': None,\n",
       "  'audio': None,\n",
       "  'function_call': None,\n",
       "  'tool_calls': [{'id': 'chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0',\n",
       "    'function': {'arguments': '{{}}', 'name': 'get_current_weather'},\n",
       "    'type': 'function'}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'chatcmpl-aad436fb6a564c3da716f2b9feb970a4',\n",
       "  'choices': [{'delta': {'content': '',\n",
       "     'function_call': None,\n",
       "     'refusal': None,\n",
       "     'role': 'assistant',\n",
       "     'tool_calls': None},\n",
       "    'finish_reason': None,\n",
       "    'index': 0,\n",
       "    'logprobs': None}],\n",
       "  'created': 1755831694,\n",
       "  'model': 'test',\n",
       "  'object': 'chat.completion.chunk',\n",
       "  'service_tier': None,\n",
       "  'system_fingerprint': None,\n",
       "  'usage': None},\n",
       " {'id': 'chatcmpl-aad436fb6a564c3da716f2b9feb970a4',\n",
       "  'choices': [{'delta': {'content': None,\n",
       "     'function_call': None,\n",
       "     'refusal': None,\n",
       "     'role': None,\n",
       "     'tool_calls': [{'index': 0,\n",
       "       'id': 'chatcmpl-tool-29e663261e524fcfa2162f4f3d76a7f0',\n",
       "       'function': {'arguments': '{', 'name': 'get_current_weather'},\n",
       "       'type': 'function'}]},\n",
       "    'finish_reason': None,\n",
       "    'index': 0,\n",
       "    'logprobs': {'content': [{'token': 'token_id:314',\n",
       "       'bytes': [32, 123],\n",
       "       'logprob': -0.00015293381875380874,\n",
       "       'top_logprobs': []}],\n",
       "     'refusal': None}}],\n",
       "  'created': 1755831694,\n",
       "  'model': 'test',\n",
       "  'object': 'chat.completion.chunk',\n",
       "  'service_tier': None,\n",
       "  'system_fingerprint': None,\n",
       "  'usage': None},\n",
       " {'id': 'chatcmpl-aad436fb6a564c3da716f2b9feb970a4',\n",
       "  'choices': [{'delta': {'content': None,\n",
       "     'function_call': None,\n",
       "     'refusal': None,\n",
       "     'role': None,\n",
       "     'tool_calls': [{'index': 0,\n",
       "       'id': None,\n",
       "       'function': {'arguments': '}', 'name': None},\n",
       "       'type': None}]},\n",
       "    'finish_reason': None,\n",
       "    'index': 0,\n",
       "    'logprobs': {'content': [{'token': 'token_id:3417',\n",
       "       'bytes': [125, 125],\n",
       "       'logprob': -3.576278118089249e-07,\n",
       "       'top_logprobs': []}],\n",
       "     'refusal': None}}],\n",
       "  'created': 1755831694,\n",
       "  'model': 'test',\n",
       "  'object': 'chat.completion.chunk',\n",
       "  'service_tier': None,\n",
       "  'system_fingerprint': None,\n",
       "  'usage': None}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from art.auto_trajectory import AutoTrajectoryContext\n",
    "\n",
    "chunks = []\n",
    "with AutoTrajectoryContext():\n",
    "    async for chunk in await client.chat.completions.create(\n",
    "        model=\"test\",\n",
    "        messages=[message],\n",
    "        tool_choice=\"required\",\n",
    "        tools=tools,\n",
    "        temperature=0.0,\n",
    "        stream=True,\n",
    "    ):\n",
    "        chunks.append(chunk.model_dump(mode=\"json\"))\n",
    "    display(art.auto_trajectory().messages_and_choices[1].model_dump(mode=\"json\"))\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c23843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-e9aa2a86eed34c15b61cd3c70c54f268',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': {'content': [{'token': 'token_id:58',\n",
       "      'bytes': [91],\n",
       "      'logprob': -0.4935532212257385,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:4913',\n",
       "      'bytes': [123, 34],\n",
       "      'logprob': -0.0022107940167188644,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:606',\n",
       "      'bytes': [110, 97, 109, 101],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:788',\n",
       "      'bytes': [34, 58],\n",
       "      'logprob': -2.0265558760002023e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:330',\n",
       "      'bytes': [32, 34],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:455',\n",
       "      'bytes': [103, 101, 116],\n",
       "      'logprob': -0.002185339340940118,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:11080',\n",
       "      'bytes': [95, 99, 117, 114, 114, 101, 110, 116],\n",
       "      'logprob': -3.576278118089249e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:69364',\n",
       "      'bytes': [95, 119, 101, 97, 116, 104, 101, 114],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:497',\n",
       "      'bytes': [34, 44],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:330',\n",
       "      'bytes': [32, 34],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:13786',\n",
       "      'bytes': [112, 97, 114, 97, 109, 101, 116, 101, 114, 115],\n",
       "      'logprob': -2.372236667724792e-05,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:788',\n",
       "      'bytes': [34, 58],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:314',\n",
       "      'bytes': [32, 123],\n",
       "      'logprob': -0.00015293381875380874,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:3417',\n",
       "      'bytes': [125, 125],\n",
       "      'logprob': -3.576278118089249e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:60',\n",
       "      'bytes': [93],\n",
       "      'logprob': -1.1920928244535389e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'token_id:151645',\n",
       "      'bytes': [],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []}],\n",
       "    'refusal': None},\n",
       "   'message': {'content': '',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'annotations': None,\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': [{'id': 'chatcmpl-tool-0580cd3326b4473280a9e6c2bc52d3fb',\n",
       "      'function': {'arguments': '{}', 'name': 'get_current_weather'},\n",
       "      'type': 'function'}],\n",
       "    'reasoning_content': None},\n",
       "   'stop_reason': None}],\n",
       " 'created': 1755829464,\n",
       " 'model': 'test',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': None,\n",
       " 'usage': {'completion_tokens': 16,\n",
       "  'prompt_tokens': 153,\n",
       "  'total_tokens': 169,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': None},\n",
       " 'prompt_logprobs': None,\n",
       " 'kv_transfer_params': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.model_dump(mode=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6803a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion.model_dump(mode=\"json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
