{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54245cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "from torch.nn.attention import flex_attention\n",
    "from torchtune.modules import attention_utils\n",
    "\n",
    "score_mod: flex_attention._score_mod_signature | None = None\n",
    "# score_mods: deque[flex_attention._score_mod_signature | None] = deque()\n",
    "\n",
    "\n",
    "# We cannot do nested compile, but flex attention only has perf benefits\n",
    "# when compiled. To insulate it from the compiler, we wrap it with\n",
    "# compiler.disable so that it can be used regardless of whether the model\n",
    "# is compiled or not, and flex attention always remains compiled.\n",
    "@torch.compiler.disable(recursive=False)\n",
    "def compile_friendly_flex_attention(\n",
    "    q: torch.Tensor,\n",
    "    k: torch.Tensor,\n",
    "    v: torch.Tensor,\n",
    "    block_mask: flex_attention.BlockMask,\n",
    ") -> torch.Tensor:\n",
    "    return attention_utils.flex_attention_compiled(\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        score_mod=score_mod,\n",
    "        block_mask=block_mask,\n",
    "        kernel_options={\n",
    "            # \"BLOCK_M\": 64,\n",
    "            # \"BLOCK_N\": 64,\n",
    "            # \"BLOCK_M1\": 64,\n",
    "            # \"BLOCK_N1\": 64,\n",
    "            \"BLOCK_M2\": 64,\n",
    "            \"BLOCK_N2\": 64,\n",
    "        },\n",
    "    )  # type: ignore\n",
    "\n",
    "\n",
    "attention_utils.compile_friendly_flex_attention = compile_friendly_flex_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art.torchtune.config import (\n",
    "    ModelConfig,\n",
    "    MetricLoggerConfig,\n",
    "    OptimizerConfig,\n",
    "    CheckpointerConfig,\n",
    "    RecipeConfig,\n",
    ")\n",
    "from art.torchtune.recipe import FullFinetuneRecipeDistributed\n",
    "import asyncio\n",
    "import glob\n",
    "import os\n",
    "\n",
    "process = await asyncio.subprocess.create_subprocess_exec(\n",
    "    \"huggingface-cli\",\n",
    "    \"download\",\n",
    "    \"Qwen/Qwen3-14B\",\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE,\n",
    ")\n",
    "stdout, _ = await process.communicate()\n",
    "checkpoint_dir = stdout.decode(\"utf-8\").splitlines()[-1].strip()\n",
    "safetensor_files = glob.glob(f\"{checkpoint_dir}/*.safetensors\")\n",
    "checkpoint_files = sorted(os.path.basename(f) for f in safetensor_files)\n",
    "print(\"checkpoint_dir\", checkpoint_dir)\n",
    "print(\"checkpoint_files\", checkpoint_files)\n",
    "\n",
    "cfg = RecipeConfig(\n",
    "    model=ModelConfig(_component_=\"torchtune.models.qwen3.qwen3_14b_instruct\"),\n",
    "    metric_logger=MetricLoggerConfig(\n",
    "        _component_=\"torchtune.training.metric_logging.StdoutLogger\"\n",
    "    ),\n",
    "    optimizer=OptimizerConfig(_component_=\"torch.optim.AdamW\"),\n",
    "    checkpointer=CheckpointerConfig(\n",
    "        _component_=\"torchtune.training.FullModelHFCheckpointer\",  # type: ignore\n",
    "        model_type=\"QWEN3\",\n",
    "        checkpoint_dir=checkpoint_dir,  # type: ignore\n",
    "        checkpoint_files=checkpoint_files,  # type: ignore\n",
    "    ),\n",
    "    output_dir=os.path.abspath(\"../.art/temporal-clue/models/002\"),\n",
    "    enable_activation_checkpointing=True,\n",
    "    enable_activation_offloading=True\n",
    ")\n",
    "\n",
    "recipe = FullFinetuneRecipeDistributed(cfg=cfg)  # type: ignore\n",
    "recipe.setup(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa545176",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_batches, batch = recipe._get_micro_batches(curr_epoch=0)\n",
    "inputs = micro_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613340d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index = inputs[\"tokens\"][0].tolist().index(151645, 3190)\n",
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inputs:\n",
    "    inputs[key] = inputs[key][:, :end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7dc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join(tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94adf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(range(-101, -1), [tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]][-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.utils import batch_to_device\n",
    "\n",
    "batch_to_device(inputs, device=recipe._device) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.attention.flex_attention import create_block_mask, BlockMask\n",
    "\n",
    "def make_block_mask(\n",
    "    group_ids: torch.Tensor,  # [B, S]  int32/64\n",
    "    parent_ids: torch.Tensor,  # [B, S]  int32/64\n",
    "    block_size: int = 256,  # Reduced from 128 to 64 to avoid OOM\n",
    ") -> BlockMask:\n",
    "    \"\"\"\n",
    "    FlexAttention equivalent of\n",
    "\n",
    "        causal_mask & (group_ids[q]==group_ids[kv]  |  parent_ids[kv]==group_ids[q])\n",
    "\n",
    "    * group_ids : id shared by all tokens of the same sampled trajectory\n",
    "    * parent_ids: id identifying the prompt that produced each token\n",
    "    \"\"\"\n",
    "    B, S = group_ids.shape  # batch, sequence length\n",
    "\n",
    "    # the closure captures the two id tensors; that's fine for torch.compile\n",
    "    def mask_mod(b, h, q_idx, kv_idx):\n",
    "        # causal constraint\n",
    "        causal = kv_idx <= q_idx\n",
    "\n",
    "        same_group = group_ids[b, q_idx] == group_ids[b, kv_idx]\n",
    "        prompt_link = parent_ids[b, q_idx] == group_ids[b, kv_idx]\n",
    "\n",
    "        return causal & (same_group | prompt_link)\n",
    "\n",
    "    return create_block_mask(\n",
    "        mask_mod,\n",
    "        B=B,\n",
    "        H=None,\n",
    "        Q_LEN=S,\n",
    "        KV_LEN=S,\n",
    "        BLOCK_SIZE=block_size,\n",
    "    )\n",
    "\n",
    "block_mask = make_block_mask(\n",
    "    group_ids=inputs[\"group_ids\"],\n",
    "    parent_ids=inputs[\"parent_ids\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tensor_memory_gb(\n",
    "    shape: tuple[int, ...],\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    binary_gb: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the memory usage of a tensor in gigabytes without creating it.\n",
    "\n",
    "    Args:\n",
    "        shape: The shape/dimensions of the tensor (e.g., (1024, 1024, 512))\n",
    "        dtype: The data type of the tensor (e.g., torch.float32, torch.float16)\n",
    "        binary_gb: If True, use binary GB (1024^3), if False use decimal GB (10^9)\n",
    "\n",
    "    Returns:\n",
    "        Memory usage in gigabytes\n",
    "    \"\"\"\n",
    "    # Calculate total number of elements\n",
    "    total_elements = 1\n",
    "    for dim in shape:\n",
    "        total_elements *= dim\n",
    "\n",
    "    # Get bytes per element based on dtype\n",
    "    bytes_per_element = torch.tensor([], dtype=dtype).element_size()\n",
    "\n",
    "    # Calculate total bytes\n",
    "    total_bytes = total_elements * bytes_per_element\n",
    "\n",
    "    # Convert to GB\n",
    "    if binary_gb:\n",
    "        gb_divisor = 1024**3  # 1 GiB = 1024^3 bytes\n",
    "    else:\n",
    "        gb_divisor = 10**9  # 1 GB = 10^9 bytes\n",
    "\n",
    "    return total_bytes / gb_divisor\n",
    "\n",
    "calculate_tensor_memory_gb((1, inputs[\"tokens\"].shape[1], inputs[\"tokens\"].shape[1]), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_bias = torch.zeros((1, inputs[\"tokens\"].shape[1], inputs[\"tokens\"].shape[1]), dtype=torch.bfloat16, device=recipe._device, requires_grad=True)\n",
    "attn_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60057d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mod = lambda score, b, h, q_idx, kv_idx: score + attn_bias[b, q_idx, kv_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mods_list = [score_mod if i % 2 == 0 else None for i in range(len(recipe._model.layers))]\n",
    "# score_mods = deque(score_mods_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mods = deque([score_mod] + [None] * (len(recipe._model.layers) - 1))\n",
    "# score_mods = deque([None] * (len(recipe._model.layers) - 1) + [score_mod])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with recipe.activations_handling_ctx:\n",
    "    hidden_states = recipe._model(\n",
    "        tokens=inputs[\"tokens\"],\n",
    "        mask=block_mask,\n",
    "        input_pos=inputs[\"input_pos\"],\n",
    "    )\n",
    "\n",
    "# del block_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.unsloth.train import shift_tensor\n",
    "from typing import cast\n",
    "\n",
    "assistant_mask = shift_tensor(inputs[\"assistant_mask\"], False)\n",
    "hidden_states = hidden_states[assistant_mask]\n",
    "next_token_ids = shift_tensor(inputs[\"tokens\"], 0)[assistant_mask]\n",
    "chunk_size = batch.dev_config.get(\"logprob_calculation_chunk_size\", 1024)\n",
    "logit_sum = torch.tensor(0.0, device=recipe._device)\n",
    "all_new_logprobs = []\n",
    "for i in range(0, hidden_states.size(0), chunk_size):\n",
    "    chunk_end = min(i + chunk_size, hidden_states.size(0))\n",
    "    # [chunk_size, hidden_size] @ [hidden_size, vocab_size]\n",
    "    logits = cast(\n",
    "        torch.Tensor, recipe._model.output(hidden_states[i:chunk_end,:])\n",
    "    )  # [chunk_size, vocab_size]\n",
    "    selected_logits = torch.gather(\n",
    "        logits, dim=-1, index=next_token_ids[i:chunk_end].unsqueeze(-1)\n",
    "    ).squeeze(\n",
    "        -1\n",
    "    )  # [chunk_size]\n",
    "    logsumexp = torch.logsumexp(logits, dim=-1)  # [chunk_size]\n",
    "    new_logprobs = selected_logits - logsumexp\n",
    "    all_new_logprobs.append(new_logprobs)\n",
    "    # loss += new_logprobs.sum()\n",
    "    logit_sum += logits.sum()\n",
    "    # loss += torch.exp(new_logprobs).sum()\n",
    "    del logits, selected_logits, logsumexp, new_logprobs\n",
    "\n",
    "new_logprobs = torch.cat(all_new_logprobs, dim=0)\n",
    "new_logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add L1 loss for attn_bias\n",
    "# loss += torch.abs(attn_bias).sum() * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# for _ in range(3):\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eea79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mods = deque([None] * (len(recipe._model.layers) - 1) + [score_mod])\n",
    "# score_mods = deque(reversed(score_mods_list))\n",
    "\n",
    "recipe.activations_handling_ctx.repack_tensors = True\n",
    "result = torch.autograd.grad(logit_sum, attn_bias, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cda42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe.activations_handling_ctx.is_first_backward_call = True\n",
    "recipe.activations_handling_ctx.repack_tensors = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8222d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562297",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_sum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ab122",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_bias.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ff824",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mod = None\n",
    "\n",
    "with recipe.activations_handling_ctx:\n",
    "    hidden_states = recipe._model(\n",
    "        tokens=inputs[\"tokens\"],\n",
    "        mask=block_mask,\n",
    "        input_pos=inputs[\"input_pos\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = hidden_states[assistant_mask]\n",
    "loss_sum = torch.tensor(0.0, device=recipe._device)\n",
    "for i in range(0, hidden_states.size(0), chunk_size):\n",
    "    chunk_end = min(i + chunk_size, hidden_states.size(0))\n",
    "    # [chunk_size, hidden_size] @ [hidden_size, vocab_size]\n",
    "    logits = cast(\n",
    "        torch.Tensor, recipe._model.output(hidden_states[i:chunk_end,:])\n",
    "    )  # [chunk_size, vocab_size]\n",
    "    selected_logits = torch.gather(\n",
    "        logits, dim=-1, index=next_token_ids[i:chunk_end].unsqueeze(-1)\n",
    "    ).squeeze(\n",
    "        -1\n",
    "    )  # [chunk_size]\n",
    "    logsumexp = torch.logsumexp(logits, dim=-1)  # [chunk_size]\n",
    "    new_logprobs = selected_logits - logsumexp\n",
    "    all_new_logprobs.append(new_logprobs)\n",
    "    loss_sum += new_logprobs.sum()\n",
    "    # logit_sum += logits.sum()\n",
    "    # loss += torch.exp(new_logprobs).sum()\n",
    "    del logits, selected_logits, logsumexp, new_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sum *= result[0].sum(dim=1)[assistant_mask].sum()\n",
    "loss_sum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9f90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c98c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention patterns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "grad_matrix = result[0][0].detach().cpu().float().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Full heatmap with log scale\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(np.log10(np.abs(grad_matrix) + 1e-10), cmap='viridis', aspect='auto')\n",
    "ax.set_title('Log10(|Gradient|) Heatmap')\n",
    "ax.set_xlabel('Key/Value Position')\n",
    "ax.set_ylabel('Query Position')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 2. Row-wise sum (gradient strength per query position)\n",
    "ax = axes[0, 1]\n",
    "row_sums = np.abs(grad_matrix).sum(axis=1)\n",
    "ax.plot(row_sums)\n",
    "ax.set_title('Gradient Strength per Query Position')\n",
    "ax.set_xlabel('Query Position')\n",
    "ax.set_ylabel('Sum of Absolute Gradients')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Column-wise sum (gradient strength per key/value position)\n",
    "ax = axes[1, 0]\n",
    "col_sums = np.abs(grad_matrix).sum(axis=0)\n",
    "ax.plot(col_sums)\n",
    "ax.set_title('Gradient Strength per Key/Value Position')\n",
    "ax.set_xlabel('Key/Value Position')\n",
    "ax.set_ylabel('Sum of Absolute Gradients')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Diagonal values (self-attention gradients)\n",
    "ax = axes[1, 1]\n",
    "diagonal = np.diag(grad_matrix)\n",
    "ax.plot(diagonal)\n",
    "ax.set_title('Diagonal Values (Self-Attention Gradients)')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Gradient Value')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print pattern analysis\n",
    "print(\"Pattern Analysis:\")\n",
    "print(f\"- Strongest gradient at position: ({np.unravel_index(np.argmax(np.abs(grad_matrix)), grad_matrix.shape)})\")\n",
    "print(f\"- Diagonal mean: {diagonal.mean():.6f}\")\n",
    "print(f\"- Off-diagonal mean: {(grad_matrix.sum() - diagonal.sum()) / (grad_matrix.size - len(diagonal)):.6f}\")\n",
    "print(f\"- Upper triangular sum: {np.triu(grad_matrix, k=1).sum():.6f}\")\n",
    "print(f\"- Lower triangular sum: {np.tril(grad_matrix, k=-1).sum():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"advantages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot((grad_matrix.sum(axis=0) + 70_000).cumsum(), color='darkgreen', linewidth=2)\n",
    "plt.title('Cumulative Sum of Gradient Values by Key/Value Position')\n",
    "plt.xlabel('Key/Value Position')\n",
    "plt.ylabel('Cumulative Sum of Gradients')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot absolute of sum of gradient values by key/value position\n",
    "col_sums = grad_matrix.sum(axis=0)\n",
    "abs_col_sums = col_sums # np.abs(col_sums)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(abs_col_sums, color='darkgreen', linewidth=2)\n",
    "plt.title('Absolute of Sum of Gradient Values by Key/Value Position')\n",
    "plt.xlabel('Key/Value Position')\n",
    "plt.ylabel('Absolute of Sum of Gradients')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max absolute sum at position: {np.argmax(abs_col_sums)} (value: {abs_col_sums.max():.6f})\")\n",
    "print(f\"Mean absolute sum: {abs_col_sums.mean():.6f}\")\n",
    "print(f\"Standard deviation: {abs_col_sums.std():.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2211b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gradient strength per query position with rolling average\n",
    "row_sums = np.abs(grad_matrix).sum(axis=1)\n",
    "\n",
    "# Calculate rolling average (window size of 50)\n",
    "window_size = 50\n",
    "rolling_avg = np.convolve(row_sums, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Create x-axis for rolling average that matches its length\n",
    "rolling_x = range(window_size//2, window_size//2 + len(rolling_avg))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plt.plot(row_sums, alpha=0.3, label='Raw gradient strength', color='lightblue')\n",
    "plt.plot(rolling_x, rolling_avg, \n",
    "         label=f'Rolling average (window={window_size})', color='darkblue', linewidth=2)\n",
    "plt.title('Gradient Strength per Query Position (with Rolling Average)')\n",
    "plt.xlabel('Query Position')\n",
    "plt.ylabel('Sum of Absolute Gradients')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6654d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0][0][-2].to(torch.float32).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from illustrate import illustrate\n",
    "\n",
    "# -44: Professor Plum\n",
    "# -39: Revolver\n",
    "# -34: Gazebo\n",
    "# -18: Ambition\n",
    "# -13: Betrayal\n",
    "# -7: Conservatory\n",
    "# -2: Lounge\n",
    "\n",
    "print(\n",
    "    illustrate(\n",
    "        list(\n",
    "            zip(\n",
    "                list(tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]),\n",
    "                result[0][0][-7].to(torch.float32).tolist(),\n",
    "                # result[0][0].sum(dim=1).squeeze(0).tolist(),\n",
    "            )\n",
    "        ),\n",
    "        gradient=\"one-dark-simple\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = result[0]\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f980e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[0][-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a81614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from illustrate import illustrate\n",
    "\n",
    "tokens = [tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]]\n",
    "logprobs = torch.exp(new_logprobs).tolist()\n",
    "print(\n",
    "    illustrate(\n",
    "        list(\n",
    "            zip(\n",
    "                tokens,\n",
    "                [0] * (len(tokens) - len(logprobs)) + logprobs,\n",
    "            )\n",
    "        ),\n",
    "        gradient=\"one-dark-simple\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf62203",
   "metadata": {},
   "outputs": [],
   "source": [
    "([0] * (len(tokens) - len(logprobs)) + logprobs)[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].sum(dim=1).squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e528f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_sum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe._optimizer.step()\n",
    "recipe._optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4abb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe._model.layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
