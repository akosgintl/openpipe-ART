{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54245cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "from torch.nn.attention import flex_attention\n",
    "from torchtune.modules import attention_utils\n",
    "\n",
    "score_mod: flex_attention._score_mod_signature | None = None\n",
    "# score_mods: deque[flex_attention._score_mod_signature | None] = deque()\n",
    "\n",
    "\n",
    "# We cannot do nested compile, but flex attention only has perf benefits\n",
    "# when compiled. To insulate it from the compiler, we wrap it with\n",
    "# compiler.disable so that it can be used regardless of whether the model\n",
    "# is compiled or not, and flex attention always remains compiled.\n",
    "@torch.compiler.disable(recursive=False)\n",
    "def compile_friendly_flex_attention(\n",
    "    q: torch.Tensor,\n",
    "    k: torch.Tensor,\n",
    "    v: torch.Tensor,\n",
    "    block_mask: flex_attention.BlockMask,\n",
    ") -> torch.Tensor:\n",
    "    return attention_utils.flex_attention_compiled(\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        score_mod=score_mod,\n",
    "        block_mask=block_mask,\n",
    "        kernel_options={\n",
    "            # \"BLOCK_M\": 64,\n",
    "            # \"BLOCK_N\": 64,\n",
    "            # \"BLOCK_M1\": 64,\n",
    "            # \"BLOCK_N1\": 64,\n",
    "            \"BLOCK_M2\": 64,\n",
    "            \"BLOCK_N2\": 64,\n",
    "        },\n",
    "    )  # type: ignore\n",
    "\n",
    "\n",
    "attention_utils.compile_friendly_flex_attention = compile_friendly_flex_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art.torchtune.config import (\n",
    "    ModelConfig,\n",
    "    MetricLoggerConfig,\n",
    "    OptimizerConfig,\n",
    "    CheckpointerConfig,\n",
    "    RecipeConfig,\n",
    ")\n",
    "from art.torchtune.recipe import FullFinetuneRecipeDistributed\n",
    "import asyncio\n",
    "import glob\n",
    "import os\n",
    "\n",
    "process = await asyncio.subprocess.create_subprocess_exec(\n",
    "    \"huggingface-cli\",\n",
    "    \"download\",\n",
    "    \"Qwen/Qwen3-14B\",\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE,\n",
    ")\n",
    "stdout, _ = await process.communicate()\n",
    "checkpoint_dir = stdout.decode(\"utf-8\").splitlines()[-1].strip()\n",
    "safetensor_files = glob.glob(f\"{checkpoint_dir}/*.safetensors\")\n",
    "checkpoint_files = sorted(os.path.basename(f) for f in safetensor_files)\n",
    "print(\"checkpoint_dir\", checkpoint_dir)\n",
    "print(\"checkpoint_files\", checkpoint_files)\n",
    "\n",
    "cfg = RecipeConfig(\n",
    "    model=ModelConfig(_component_=\"torchtune.models.qwen3.qwen3_14b_instruct\"),\n",
    "    metric_logger=MetricLoggerConfig(\n",
    "        _component_=\"torchtune.training.metric_logging.StdoutLogger\"\n",
    "    ),\n",
    "    optimizer=OptimizerConfig(_component_=\"torch.optim.AdamW\"),\n",
    "    checkpointer=CheckpointerConfig(\n",
    "        _component_=\"torchtune.training.FullModelHFCheckpointer\",  # type: ignore\n",
    "        model_type=\"QWEN3\",\n",
    "        checkpoint_dir=checkpoint_dir,  # type: ignore\n",
    "        checkpoint_files=checkpoint_files,  # type: ignore\n",
    "    ),\n",
    "    output_dir=os.path.abspath(\"../.art/temporal-clue/models/002\"),\n",
    "    enable_activation_checkpointing=True,\n",
    "    enable_activation_offloading=True\n",
    ")\n",
    "\n",
    "recipe = FullFinetuneRecipeDistributed(cfg=cfg)  # type: ignore\n",
    "recipe.setup(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa545176",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_batches, batch = recipe._get_micro_batches(curr_epoch=0)\n",
    "inputs = micro_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613340d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ae8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join(tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index = inputs[\"tokens\"][0].tolist().index(151645, 3190)\n",
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inputs:\n",
    "    inputs[key] = inputs[key][:, :end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94adf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(range(-101, -1), [tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]][-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.utils import batch_to_device\n",
    "\n",
    "batch_to_device(inputs, device=recipe._device) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.attention.flex_attention import create_block_mask, BlockMask\n",
    "\n",
    "def make_block_mask(\n",
    "    group_ids: torch.Tensor,  # [B, S]  int32/64\n",
    "    parent_ids: torch.Tensor,  # [B, S]  int32/64\n",
    "    block_size: int = 256,  # Reduced from 128 to 64 to avoid OOM\n",
    ") -> BlockMask:\n",
    "    \"\"\"\n",
    "    FlexAttention equivalent of\n",
    "\n",
    "        causal_mask & (group_ids[q]==group_ids[kv]  |  parent_ids[kv]==group_ids[q])\n",
    "\n",
    "    * group_ids : id shared by all tokens of the same sampled trajectory\n",
    "    * parent_ids: id identifying the prompt that produced each token\n",
    "    \"\"\"\n",
    "    B, S = group_ids.shape  # batch, sequence length\n",
    "\n",
    "    # the closure captures the two id tensors; that's fine for torch.compile\n",
    "    def mask_mod(b, h, q_idx, kv_idx):\n",
    "        # causal constraint\n",
    "        causal = kv_idx <= q_idx\n",
    "\n",
    "        same_group = group_ids[b, q_idx] == group_ids[b, kv_idx]\n",
    "        prompt_link = parent_ids[b, q_idx] == group_ids[b, kv_idx]\n",
    "\n",
    "        return causal & (same_group | prompt_link)\n",
    "\n",
    "    return create_block_mask(\n",
    "        mask_mod,\n",
    "        B=B,\n",
    "        H=None,\n",
    "        Q_LEN=S,\n",
    "        KV_LEN=S,\n",
    "        BLOCK_SIZE=block_size,\n",
    "    )\n",
    "\n",
    "block_mask = make_block_mask(\n",
    "    group_ids=inputs[\"group_ids\"],\n",
    "    parent_ids=inputs[\"parent_ids\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tensor_memory_gb(\n",
    "    shape: tuple[int, ...],\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    binary_gb: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the memory usage of a tensor in gigabytes without creating it.\n",
    "\n",
    "    Args:\n",
    "        shape: The shape/dimensions of the tensor (e.g., (1024, 1024, 512))\n",
    "        dtype: The data type of the tensor (e.g., torch.float32, torch.float16)\n",
    "        binary_gb: If True, use binary GB (1024^3), if False use decimal GB (10^9)\n",
    "\n",
    "    Returns:\n",
    "        Memory usage in gigabytes\n",
    "    \"\"\"\n",
    "    # Calculate total number of elements\n",
    "    total_elements = 1\n",
    "    for dim in shape:\n",
    "        total_elements *= dim\n",
    "\n",
    "    # Get bytes per element based on dtype\n",
    "    bytes_per_element = torch.tensor([], dtype=dtype).element_size()\n",
    "\n",
    "    # Calculate total bytes\n",
    "    total_bytes = total_elements * bytes_per_element\n",
    "\n",
    "    # Convert to GB\n",
    "    if binary_gb:\n",
    "        gb_divisor = 1024**3  # 1 GiB = 1024^3 bytes\n",
    "    else:\n",
    "        gb_divisor = 10**9  # 1 GB = 10^9 bytes\n",
    "\n",
    "    return total_bytes / gb_divisor\n",
    "\n",
    "calculate_tensor_memory_gb((1, inputs[\"tokens\"].shape[1], inputs[\"tokens\"].shape[1]), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_bias = torch.zeros((1, inputs[\"tokens\"].shape[1], inputs[\"tokens\"].shape[1]), dtype=torch.bfloat16, device=recipe._device, requires_grad=True)\n",
    "attn_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60057d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mod = lambda score, b, h, q_idx, kv_idx: score + attn_bias[b, q_idx, kv_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mods_list = [score_mod if i % 2 == 0 else None for i in range(len(recipe._model.layers))]\n",
    "# score_mods = deque(score_mods_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mods = deque([score_mod] + [None] * (len(recipe._model.layers) - 1))\n",
    "# score_mods = deque([None] * (len(recipe._model.layers) - 1) + [score_mod])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with recipe.activations_handling_ctx:\n",
    "    hidden_states = recipe._model(\n",
    "        tokens=inputs[\"tokens\"],\n",
    "        mask=block_mask,\n",
    "        input_pos=inputs[\"input_pos\"],\n",
    "    )\n",
    "\n",
    "del block_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.unsloth.train import shift_tensor\n",
    "from typing import cast\n",
    "\n",
    "assistant_mask = shift_tensor(inputs[\"assistant_mask\"], False)\n",
    "hidden_states = hidden_states[assistant_mask]\n",
    "next_token_ids = shift_tensor(inputs[\"tokens\"], 0)[assistant_mask]\n",
    "chunk_size = batch.dev_config.get(\"logprob_calculation_chunk_size\", 1024)\n",
    "loss = torch.tensor(0.0, device=recipe._device)\n",
    "all_new_logprobs = []\n",
    "for i in range(0, hidden_states.size(0), chunk_size):\n",
    "    chunk_end = min(i + chunk_size, hidden_states.size(0))\n",
    "    # [chunk_size, hidden_size] @ [hidden_size, vocab_size]\n",
    "    logits = cast(\n",
    "        torch.Tensor, recipe._model.output(hidden_states[i:chunk_end,:])\n",
    "    )  # [chunk_size, vocab_size]\n",
    "    selected_logits = torch.gather(\n",
    "        logits, dim=-1, index=next_token_ids[i:chunk_end].unsqueeze(-1)\n",
    "    ).squeeze(\n",
    "        -1\n",
    "    )  # [chunk_size]\n",
    "    logsumexp = torch.logsumexp(logits, dim=-1)  # [chunk_size]\n",
    "    new_logprobs = selected_logits - logsumexp\n",
    "    all_new_logprobs.append(new_logprobs)\n",
    "    # loss += new_logprobs.sum()\n",
    "    loss += logits.sum()\n",
    "    # loss += torch.exp(new_logprobs).sum()\n",
    "    del logits, selected_logits, logsumexp, new_logprobs\n",
    "\n",
    "new_logprobs = torch.cat(all_new_logprobs, dim=0)\n",
    "new_logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add L1 loss for attn_bias\n",
    "# loss += torch.abs(attn_bias).sum() * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# for _ in range(3):\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19feeb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mods = deque([None] * (len(recipe._model.layers) - 1) + [score_mod])\n",
    "# score_mods = deque(reversed(score_mods_list))\n",
    "result = torch.autograd.grad(loss, attn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd429fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84eaffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0][0].sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the gradient matrix - sample it down to speed up visualization\n",
    "grad_matrix = result[0][0].to(torch.float16).cpu().numpy()\n",
    "\n",
    "# Subsample the matrix for faster visualization\n",
    "max_size = 1000  # Limit to 1000x1000 for performance\n",
    "if grad_matrix.shape[0] > max_size or grad_matrix.shape[1] > max_size:\n",
    "    # Calculate step sizes for subsampling\n",
    "    step_y = max(1, grad_matrix.shape[0] // max_size)\n",
    "    step_x = max(1, grad_matrix.shape[1] // max_size)\n",
    "    grad_matrix_viz = grad_matrix[::step_y, ::step_x]\n",
    "    print(f\"Subsampled from {grad_matrix.shape} to {grad_matrix_viz.shape} for visualization\")\n",
    "else:\n",
    "    grad_matrix_viz = grad_matrix\n",
    "\n",
    "# Create a heatmap with appropriate sizing\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Use rasterized rendering for better performance with large matrices\n",
    "im = plt.imshow(grad_matrix_viz, cmap='RdBu_r', aspect='auto', interpolation='nearest', rasterized=True)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im, label='Gradient Value')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Attention Head Dimension')\n",
    "plt.ylabel('Token Position')\n",
    "plt.title(f'Attention Bias Gradients Heatmap\\nOriginal Shape: {grad_matrix.shape}')\n",
    "\n",
    "# Make it more readable\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.where(\n",
    "    torch.logical_and(torch.isinf(result[0][0]), result[0][0] > 0),\n",
    "    result[0][0].max(),\n",
    "    result[0][0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl.Series(result[0][0].reshape(-1).to(torch.float32).cpu().numpy()).to_frame(\n",
    "    \"value\"\n",
    ").filter(pl.col(\"value\") != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series(torch.abs(result[0][0].sum(dim=1)).to(torch.float16).cpu().numpy()).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from illustrate import illustrate\n",
    "\n",
    "# -44: Professor Plum\n",
    "# -39: Revolver\n",
    "# -34: Gazebo\n",
    "# -18: Ambition\n",
    "# -13: Betrayal\n",
    "# -7: Conservatory\n",
    "# -2: Lounge\n",
    "\n",
    "print(\n",
    "    illustrate(\n",
    "        list(\n",
    "            zip(\n",
    "                list(tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]),\n",
    "                result[0][0][-22].to(torch.float32).tolist(),\n",
    "                # result[0][0].sum(dim=1).squeeze(0).tolist(),\n",
    "            )\n",
    "        ),\n",
    "        gradient=\"one-dark-simple\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from illustrate import illustrate\n",
    "\n",
    "tokens = [tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]]\n",
    "logprobs = torch.exp(new_logprobs).tolist()\n",
    "print(\n",
    "    illustrate(\n",
    "        list(\n",
    "            zip(\n",
    "                tokens,\n",
    "                [0] * (len(tokens) - len(logprobs)) + logprobs,\n",
    "            )\n",
    "        ),\n",
    "        gradient=\"one-dark-simple\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf62203",
   "metadata": {},
   "outputs": [],
   "source": [
    "([0] * (len(tokens) - len(logprobs)) + logprobs)[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].sum(dim=1).squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e528f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode(token_id) for token_id in inputs[\"tokens\"][0]][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe._optimizer.step()\n",
    "recipe._optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4abb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe._model.layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
